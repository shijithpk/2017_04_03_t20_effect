{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Has Twenty20 changed the way test matches are played?\n",
    "This code was written for a wionews.com article, [\"Is T20 ruining Test cricket?\"](http://www.wionews.com/cricket/analysis-is-t20-format-ruining-test-cricket-14150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The first part of the code involves scraping the data from cricketarchive.com\n",
    "#this is just some libraries being imported that we will use later in scraping the website\n",
    "\n",
    "from random import randint\n",
    "import time\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1990,\n",
       " 1991,\n",
       " 1992,\n",
       " 1993,\n",
       " .,\n",
       " .,\n",
       " .,\n",
       " 2013,\n",
       " 2014,\n",
       " 2015,\n",
       " 2016,\n",
       " 2017]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get a list of years\n",
    "list_orig = range(1990,2018)\n",
    "list_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1990', '1990-91', '1991', '1991-92', '1992', '1992-93', '1993', '1993-94', '1994', '1994-95', '1995', '1995-96', '1996', '1996-97', '1997', '1997-98', '1998', '1998-99', '1999', '1999-00', '2000', '2000-01', '2001', '2001-02', '2002', '2002-03', '2003', '2003-04', '2004', '2004-05', '2005', '2005-06', '2006', '2006-07', '2007', '2007-08', '2008', '2008-09', '2009', '2009-10', '2010', '2010-11', '2011', '2011-12', '2012', '2012-13', '2013', '2013-14', '2014', '2014-15', '2015', '2015-16', '2016', '2016-17', '2017']\n"
     ]
    }
   ],
   "source": [
    "#kind of looked at the url structure of the website, created this list because some of the urls contain these substrings\n",
    "#and would help me later in scraping the website\n",
    "#am yet to learn how to use full-blown scraping frameworks such as scrapy, will make that a next step\n",
    "\n",
    "list_new = []\n",
    "for i in range(0,len(list_orig)):\n",
    "    try:\n",
    "        m = str(list_orig[i])\n",
    "        list_new.append(m)\n",
    "        i_next = list_orig[i+1]\n",
    "        j = str(i_next)\n",
    "        k = j[-2:]\n",
    "        l = m + \"-\" + k\n",
    "        list_new.append(l)\n",
    "    except IndexError:\n",
    "        pass\n",
    "print list_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://cricketarchive.com/Archive/Seasons/Lists/1990_t_Match_List.html', 'http://cricketarchive.com/Archive/Seasons/Lists/1990-91_t_Match_List.html', 'http://cricketarchive.com/Archive/Seasons/Lists/1991_t_Match_List.html', 'http://cricketarchive.com/Archive/Seasons/Lists/1991-92_t_Match_List.html', '...',  'http://cricketarchive.com/Archive/Seasons/Lists/2015-16_t_Match_List.html', 'http://cricketarchive.com/Archive/Seasons/Lists/2016_t_Match_List.html', 'http://cricketarchive.com/Archive/Seasons/Lists/2016-17_t_Match_List.html', 'http://cricketarchive.com/Archive/Seasons/Lists/2017_t_Match_List.html']\n"
     ]
    }
   ],
   "source": [
    "#again more prep work creating urls before the actual scraping begins\n",
    "\n",
    "test_urls = []\n",
    "odi_urls = []\n",
    "t20_urls = []\n",
    "\n",
    "for year in list_new:\n",
    "    a = \"http://cricketarchive.com/Archive/Seasons/Lists/\" + year + \"_t_Match_List.html\"\n",
    "    test_urls.append(a)\n",
    "    b = \"http://cricketarchive.com/Archive/Seasons/Lists/\" + year + \"_o_Match_List.html\"\n",
    "    odi_urls.append(b)\n",
    "    c = \"http://cricketarchive.com/Archive/Seasons/Lists/\" + year + \"_itt_Match_List.html\"\n",
    "    t20_urls.append(c)\n",
    "\n",
    "print test_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://cricketarchive.com/Archive/Seasons/Lists/1990_itt_Match_List.html', 'http://cricketarchive.com/Archive/Seasons/Lists/1990-91_itt_Match_List.html', 'http://cricketarchive.com/Archive/Seasons/Lists/1991_itt_Match_List.html', 'http://cricketarchive.com/Archive/Seasons/Lists/1991-92_itt_Match_List.html', '....', 'http://cricketarchive.com/Archive/Seasons/Lists/2014-15_itt_Match_List.html', 'http://cricketarchive.com/Archive/Seasons/Lists/2015_itt_Match_List.html', 'http://cricketarchive.com/Archive/Seasons/Lists/2015-16_itt_Match_List.html', 'http://cricketarchive.com/Archive/Seasons/Lists/2016_itt_Match_List.html', 'http://cricketarchive.com/Archive/Seasons/Lists/2016-17_itt_Match_List.html', 'http://cricketarchive.com/Archive/Seasons/Lists/2017_itt_Match_List.html']\n"
     ]
    }
   ],
   "source": [
    "print t20_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6853f9f51f302ae418c8c40f588b1a34\n"
     ]
    }
   ],
   "source": [
    "#ok so this is where some of the scraping work begins\n",
    "#most of the code below taken from https://kazuar.github.io/scraping-tutorial/ and\n",
    "#https://brennan.io/2016/03/02/logging-in-with-requests/\n",
    "#put my username and password in a text file, the code picks it up from there, logs in to the website\n",
    "# creates a session that we'll be using for the rest of the time we're scraping data\n",
    "\n",
    "\n",
    "import requests\n",
    "import re\n",
    "\n",
    "session_requests = requests.session()\n",
    "login_url = \"https://my.cricketarchive.com/\"\n",
    "result = session_requests.get(login_url)\n",
    "sourcez = result.text\n",
    "\n",
    "\n",
    "csrf_hash2 =str((re.findall(r\"var csrfHash = '(.*)';\", sourcez))[0]) \n",
    "\n",
    "f = open('passwords_ignore.txt',\"r\")\n",
    "lines = f.readlines()\n",
    "emailid = lines[0].strip()\n",
    "passwd = lines[1].strip()\n",
    "\n",
    "payload = {\"email\": emailid,\"password\": passwd,\"vo-action\": \"login\",\"csrfHash\": csrf_hash2}\n",
    "\n",
    "print csrf_hash2\n",
    "\n",
    "response = session_requests.post(login_url,data = payload,headers = dict(referer=login_url))\n",
    "\n",
    "#r = session_requests.get('http://cricketarchive.com/Archive/Seasons/Lists/2016_itt_Match_List.html')\n",
    "#print r.text\n",
    "#this is to test if we've logged in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = session_requests.get('http://cricketarchive.com/Archive/Seasons/Lists/2016_itt_Match_List.html')\n",
    "print r.text\n",
    "#this is to test if we've logged in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this creates folders where we'll be downloading the initial set of pages\n",
    "#this initial set of pages will then be used to generate the actual set of pages with data \n",
    "#on each match and its scoreboard\n",
    "\n",
    "import os\n",
    "\n",
    "folders = [\"test\",\"odi\",\"t20\"]\n",
    "for folder in folders:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404\n"
     ]
    }
   ],
   "source": [
    "r = session_requests.get('http://cricketarchive.com/Archive/Seasons/Lists/1990_itt_Match_List.html')\n",
    "print r.status_code\n",
    "#r.raise_for_status()\n",
    "#this is to test what status code we're getting for pages that don't really exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n",
      "55\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "#just some checking\n",
    "print len(odi_urls)\n",
    "print len(test_urls)\n",
    "print len(t20_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url being processed: http://cricketarchive.com/Archive/Seasons/Lists/1990_itt_Match_List.html\n",
      "404 error here\n",
      "url being processed: http://cricketarchive.com/Archive/Seasons/Lists/1990-91_itt_Match_List.html\n",
      "404 error here\n",
      "url being processed: http://cricketarchive.com/Archive/Seasons/Lists/1991_itt_Match_List.html\n",
      "404 error here\n",
      ".\n",
      ".\n",
      ".\n",
      "url being processed: http://cricketarchive.com/Archive/Seasons/Lists/2016-17_itt_Match_List.html\n",
      "http://cricketarchive.com/Archive/Seasons/Lists/2016-17_itt_Match_List.html successfully written\n",
      "url being processed: http://cricketarchive.com/Archive/Seasons/Lists/2017_itt_Match_List.html\n",
      "404 error here\n",
      "t20s done\n"
     ]
    }
   ],
   "source": [
    "#what this code does is download a page for each cricket season, each of these 'season' pages have links to pages for \n",
    "#the various ODI, T20 and Test matches played in that season\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "for url in t20_urls:\n",
    "    print 'url being processed: ' + url\n",
    "    responsex = session_requests.get(url)\n",
    "    if responsex.status_code == 404:\n",
    "        print '404 error here'\n",
    "        time.sleep(10)\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            last = (re.findall(r\"http://cricketarchive.com/Archive/Seasons/Lists/(.*)\", url))[0]\n",
    "            filenamey = 't20/' + last\n",
    "            fnamez = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenamey)\n",
    "            with open(fnamez, \"w\") as filex:\n",
    "                filex.write(responsex.content)\n",
    "            print url + ' successfully written'\n",
    "            delay = randint(10,15)\n",
    "            time.sleep(delay)\n",
    "        except:\n",
    "            print \"Something went wrong here.\"\n",
    "print 't20s' + ' done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url being processed: http://cricketarchive.com/Archive/Seasons/Lists/1990_o_Match_List.html\n",
      "http://cricketarchive.com/Archive/Seasons/Lists/1990_o_Match_List.html successfully written\n",
      "url being processed: http://cricketarchive.com/Archive/Seasons/Lists/1990-91_o_Match_List.html\n",
      "http://cricketarchive.com/Archive/Seasons/Lists/1990-91_o_Match_List.html successfully written\n",
      ".\n",
      ".\n",
      ".\n",
      "url being processed: http://cricketarchive.com/Archive/Seasons/Lists/2016-17_o_Match_List.html\n",
      "http://cricketarchive.com/Archive/Seasons/Lists/2016-17_o_Match_List.html successfully written\n",
      "url being processed: http://cricketarchive.com/Archive/Seasons/Lists/2017_o_Match_List.html\n",
      "404 error here\n",
      "odis done\n"
     ]
    }
   ],
   "source": [
    "#this gets all the match lists for ODIs\n",
    "\n",
    "for url in odi_urls:\n",
    "    print 'url being processed: ' + url\n",
    "    responsex = session_requests.get(url)\n",
    "    if responsex.status_code == 404:\n",
    "        print '404 error here'\n",
    "        time.sleep(10)\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            last = (re.findall(r\"http://cricketarchive.com/Archive/Seasons/Lists/(.*)\", url))[0]\n",
    "            filenamey = 'odi/' + last\n",
    "            fnamez = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenamey)\n",
    "            with open(fnamez, \"w\") as filex:\n",
    "                filex.write(responsex.content)\n",
    "            print url + ' successfully written'\n",
    "            delay = randint(10,15)\n",
    "            time.sleep(delay)\n",
    "        except:\n",
    "            print \"Something went wrong here.\"\n",
    "print 'odis' + ' done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url being processed: http://cricketarchive.com/Archive/Seasons/Lists/1990_t_Match_List.html\n",
      "http://cricketarchive.com/Archive/Seasons/Lists/1990_t_Match_List.html successfully written\n",
      "url being processed: http://cricketarchive.com/Archive/Seasons/Lists/1990-91_t_Match_List.html\n",
      "http://cricketarchive.com/Archive/Seasons/Lists/1990-91_t_Match_List.html successfully written\n",
      ".\n",
      ".\n",
      ".\n",
      "url being processed: http://cricketarchive.com/Archive/Seasons/Lists/1995-96_t_Match_List.html\n",
      "http://cricketarchive.com/Archive/Seasons/Lists/1995-96_t_Match_List.html successfully written\n",
      "url being processed: http://cricketarchive.com/Archive/Seasons/Lists/1996_t_Match_List.html\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='cricketarchive.com', port=80): Max retries exceeded with url: /Archive/Seasons/Lists/1996_t_Match_List.html (Caused by NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x7f637ec62350>: Failed to establish a new connection: [Errno 110] Connection timed out',))",
     "output_type": "error",
     "traceback": [
    ".\n",
     "edited some of this traceback out.\n",
     ".\n"
      ]
    }
   ],
   "source": [
    "#this gets all the match lists for tests\n",
    "\n",
    "for url in test_urls:\n",
    "    print 'url being processed: ' + url\n",
    "    responsex = session_requests.get(url)\n",
    "    if responsex.status_code == 404:\n",
    "        print '404 error here'\n",
    "        time.sleep(10)\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            last = (re.findall(r\"http://cricketarchive.com/Archive/Seasons/Lists/(.*)\", url))[0]\n",
    "            filenamey = 'test/' + last\n",
    "            fnamez = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenamey)\n",
    "            with open(fnamez, \"w\") as filex:\n",
    "                filex.write(responsex.content)\n",
    "            print url + ' successfully written'\n",
    "            #delay = randint(10,15)\n",
    "            time.sleep(10)\n",
    "        except:\n",
    "            print \"Something went wrong here.\"\n",
    "print 'tests' + ' done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_urls.index(\"http://cricketarchive.com/Archive/Seasons/Lists/1996_t_Match_List.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url being processed: http://cricketarchive.com/Archive/Seasons/Lists/1996_t_Match_List.html\n",
      "http://cricketarchive.com/Archive/Seasons/Lists/1996_t_Match_List.html successfully written\n",
      "url being processed: http://cricketarchive.com/Archive/Seasons/Lists/1996-97_t_Match_List.html\n",
      "http://cricketarchive.com/Archive/Seasons/Lists/1996-97_t_Match_List.html successfully written\n",
      ".\n",
      ".\n",
      ".\n",
      "url being processed: http://cricketarchive.com/Archive/Seasons/Lists/2016-17_t_Match_List.html\n",
      "http://cricketarchive.com/Archive/Seasons/Lists/2016-17_t_Match_List.html successfully written\n",
      "url being processed: http://cricketarchive.com/Archive/Seasons/Lists/2017_t_Match_List.html\n",
      "404 error here\n",
      "tests done\n"
     ]
    }
   ],
   "source": [
    "#the previous code block stopped midway, this resumes the scraping for the test match lists\n",
    "\n",
    "for url in test_urls[12:]:\n",
    "    print 'url being processed: ' + url\n",
    "    responsex = session_requests.get(url)\n",
    "    if responsex.status_code == 404:\n",
    "        print '404 error here'\n",
    "        time.sleep(10)\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            last = (re.findall(r\"http://cricketarchive.com/Archive/Seasons/Lists/(.*)\", url))[0]\n",
    "            filenamey = 'test/' + last\n",
    "            fnamez = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenamey)\n",
    "            with open(fnamez, \"w\") as filex:\n",
    "                filex.write(responsex.content)\n",
    "            print url + ' successfully written'\n",
    "            delay = randint(10,15)\n",
    "            time.sleep(10)\n",
    "        except:\n",
    "            print \"Something went wrong here.\"\n",
    "print 'tests' + ' done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this creates the folders into which the scoreboards for tests, ODIs and T20 internationals will be downloaded\n",
    "\n",
    "folders = [\"test_cards\",\"odi_cards\",\"t20_cards\"]\n",
    "for folder in folders:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2004-05_itt_Match_List.html',\n",
       " '2005-06_itt_Match_List.html',\n",
       " '2005_itt_Match_List.html',\n",
       " '2006-07_itt_Match_List.html',\n",
       " '2006_itt_Match_List.html',\n",
       " '2007-08_itt_Match_List.html',\n",
       " '2007_itt_Match_List.html',\n",
       " '2008-09_itt_Match_List.html',\n",
       " '2008_itt_Match_List.html',\n",
       " '2009-10_itt_Match_List.html',\n",
       " '2009_itt_Match_List.html',\n",
       " '2010-11_itt_Match_List.html',\n",
       " '2010_itt_Match_List.html',\n",
       " '2011-12_itt_Match_List.html',\n",
       " '2011_itt_Match_List.html',\n",
       " '2012-13_itt_Match_List.html',\n",
       " '2012_itt_Match_List.html',\n",
       " '2013-14_itt_Match_List.html',\n",
       " '2013_itt_Match_List.html',\n",
       " '2014-15_itt_Match_List.html',\n",
       " '2014_itt_Match_List.html',\n",
       " '2015-16_itt_Match_List.html',\n",
       " '2015_itt_Match_List.html',\n",
       " '2016-17_itt_Match_List.html',\n",
       " '2016_itt_Match_List.html']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"t20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startingtest\n",
      "processing test/1997_t_Match_List.html\n",
      "finished test/1997_t_Match_List.html\n",
      "processing test/2005_t_Match_List.html\n",
      "finished test/2005_t_Match_List.html\n",
      ".\n",
      ".\n",
      ".\n",
      "processing test/2016_t_Match_List.html\n",
      "finished test/2016_t_Match_List.html\n",
      "finished test\n",
      "startingodi\n",
      "processing odi/1997_o_Match_List.html\n",
      "finished odi/1997_o_Match_List.html\n",
      ".\n",
      ".\n",
      ".\n",
      "processing t20/2016_itt_Match_List.html\n",
      "finished t20/2016_itt_Match_List.html\n",
      "finished t20\n",
      "done and done\n"
     ]
    }
   ],
   "source": [
    "#this allows us to scrape the urls for each match scoreboard\n",
    "\n",
    "from lxml import html\n",
    "import csv\n",
    "import os\n",
    "\n",
    "headings = ['format','number','date','year','team1','team2','ground','url']\n",
    "with open(\"url_store.csv\", \"w\") as filem:   \n",
    "    wr = csv.writer(filem, delimiter = ',' , quotechar = '\"' )\n",
    "    wr.writerow(headings)\n",
    "\n",
    "for i in [\"test\",\"odi\",\"t20\"]:\n",
    "    d = 0\n",
    "    print \"starting\" + i\n",
    "    for j in os.listdir(i):\n",
    "        filex = i + \"/\" + j\n",
    "        print \"processing \" + filex\n",
    "        filey =  os.path.join(os.path.dirname(os.path.realpath('__file__')), filex)\n",
    "        with open(filey) as filez:\n",
    "            sourcex = filez.read()\n",
    "            treex = html.document_fromstring(sourcex)\n",
    "            nodes = treex.xpath(\"//td[@valign='top']/a[contains(@href,'/Scorecards/')]\")\n",
    "            for node in nodes:\n",
    "                row_list = []\n",
    "                format = i\n",
    "                row_list.append(format) #hmm, noticed this later, seems format is a reserved word, didn't give me errors though\n",
    "                d += 1\n",
    "                zrs = str(d).zfill(4)\n",
    "                number = i + \"_\" + zrs\n",
    "                row_list.append(number)\n",
    "                date = node.xpath(\"preceding::td[2]/text()\")[0]\n",
    "                row_list.append(date)\n",
    "                row_list.append(date[-4:])\n",
    "                opponents = node.xpath(\"text()\")[0]\n",
    "                opp_list = opponents.split(' v ')\n",
    "                row_list.append(opp_list[0])\n",
    "                row_list.append(opp_list[1])\n",
    "                ground = node.xpath(\"following::td[1]/a/text()\")[0]\n",
    "                row_list.append(ground)\n",
    "                url = node.xpath(\"@href\")[0]\n",
    "                row_list.append(url)\n",
    "                with open(\"url_store.csv\", \"a\") as filen:\n",
    "                    wrz = csv.writer(filen, delimiter = ',' , quotechar = '\"' )\n",
    "                    wrz.writerow(row_list)\n",
    "        print \"finished \" + filex\n",
    "    print \"finished \" + i\n",
    "\n",
    "filen.close()\n",
    "print 'done and done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing test_0001\n",
      "finished test_0001\n",
      "processing test_0002\n",
      "finished test_0002\n",
      "processing test_0003\n",
      "finished test_0003\n",
      "processing test_0004\n",
      "finished test_0004\n",
      ".\n",
      ".\n",
      ".\n",
      "processing t20_0616\n",
      "finished t20_0616\n",
      "processing t20_0617\n",
      "finished t20_0617\n",
      "finished dloading cards\n"
     ]
    }
   ],
   "source": [
    "#this generates the list of urls to each scoreboard, be it ODI, T20 or Test\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('url_store.csv')\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    print \"processing \" + row['number']\n",
    "    urlx = \"http://cricketarchive.com\" + row['url']\n",
    "    responseq = session_requests.get(urlx)\n",
    "    filenamey = row['format'] + \"_cards/\" + row['number'] + \".html\"\n",
    "    fnamez = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenamey)\n",
    "    with open(fnamez, \"w\") as filex:\n",
    "        filex.write(responseq.content)\n",
    "    print \"finished \" + row['number']\n",
    "    delay = randint(10,15)\n",
    "    time.sleep(delay)\n",
    "\n",
    "print 'finished dloading cards'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#I'll be honest, some of the code blocks below are just pure nonsense\n",
    "# I was trying to figure out a way to find the country a ground is located\n",
    "#that information wasn't really in the first set of pages I downloaded\n",
    "# so I had to download the webpage for each ground, where there is an entry mentioning the country the ground\n",
    "#is located in, scrape that entry and put that information back in the original csv file\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('url_store.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>format</th>\n",
       "      <th>number</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>team1</th>\n",
       "      <th>team2</th>\n",
       "      <th>ground</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>test_0001</td>\n",
       "      <td>05 Jun 1997</td>\n",
       "      <td>1997</td>\n",
       "      <td>England</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Edgbaston, Birmingham</td>\n",
       "      <td>/Archive/Scorecards/64/64098.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>test_0002</td>\n",
       "      <td>19 Jun 1997</td>\n",
       "      <td>1997</td>\n",
       "      <td>England</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Lord's Cricket Ground, St John's Wood</td>\n",
       "      <td>/Archive/Scorecards/64/64171.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>test_0003</td>\n",
       "      <td>03 Jul 1997</td>\n",
       "      <td>1997</td>\n",
       "      <td>England</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Old Trafford, Manchester</td>\n",
       "      <td>/Archive/Scorecards/64/64259.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test</td>\n",
       "      <td>test_0004</td>\n",
       "      <td>24 Jul 1997</td>\n",
       "      <td>1997</td>\n",
       "      <td>England</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Headingley, Leeds</td>\n",
       "      <td>/Archive/Scorecards/64/64378.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test</td>\n",
       "      <td>test_0005</td>\n",
       "      <td>02 Aug 1997</td>\n",
       "      <td>1997</td>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>India</td>\n",
       "      <td>R Premadasa Stadium, Colombo</td>\n",
       "      <td>/Archive/Scorecards/64/64422.html</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  format     number         date  year      team1      team2  \\\n",
       "0   test  test_0001  05 Jun 1997  1997    England  Australia   \n",
       "1   test  test_0002  19 Jun 1997  1997    England  Australia   \n",
       "2   test  test_0003  03 Jul 1997  1997    England  Australia   \n",
       "3   test  test_0004  24 Jul 1997  1997    England  Australia   \n",
       "4   test  test_0005  02 Aug 1997  1997  Sri Lanka      India   \n",
       "\n",
       "                                  ground                                url  \n",
       "0                  Edgbaston, Birmingham  /Archive/Scorecards/64/64098.html  \n",
       "1  Lord's Cricket Ground, St John's Wood  /Archive/Scorecards/64/64171.html  \n",
       "2               Old Trafford, Manchester  /Archive/Scorecards/64/64259.html  \n",
       "3                      Headingley, Leeds  /Archive/Scorecards/64/64378.html  \n",
       "4           R Premadasa Stadium, Colombo  /Archive/Scorecards/64/64422.html  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Edgbaston, Birmingham', \"Lord's Cricket Ground, St John's Wood\",\n",
       "       'Old Trafford, Manchester', 'Headingley, Leeds',\n",
       "       'R Premadasa Stadium, Colombo', 'Trent Bridge, Nottingham',\n",
       "       'Sinhalese Sports Club Ground, Colombo',\n",
       "       \"The Foster's Oval, Kennington\",\n",
       "       'Riverside Ground, Chester-le-Street', 'Asgiriya Stadium, Kandy',\n",
       "       'The Brit Oval, Kennington', 'National Stadium, Karachi',\n",
       ".\n",
       ".\n",
       ".\n",
       "       'Greater Noida Sports Complex Ground, Greater Noida',\n",
       "       'Moses Mabhida Stadium, Durban',\n",
       "       'Central Broward Regional Park Stadium Turf Ground, Lauderhill',\n",
       "       'ANZ Stadium, Sydney', 'Sylhet Stadium, Sylhet',\n",
       "       'Wanderers Cricket Ground, Windhoek', 'Magheramason, Bready',\n",
       "       'Simonds Stadium, Geelong'], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this gets  list of all the grounds at which matches -- ODI, T20 and Test -- have taken place over the past 25 years\n",
    "\n",
    "import numpy\n",
    "\n",
    "#finding uniques in a column\n",
    "df['ground'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startingtest\n",
      "processing test/1997_t_Match_List.html\n",
      "finished test/1997_t_Match_List.html\n",
      "processing test/2005_t_Match_List.html\n",
      "finished test/2005_t_Match_List.html\n",
      ".\n",
      ".\n",
      ".\n",
      "processing t20/2016_itt_Match_List.html\n",
      "finished t20/2016_itt_Match_List.html\n",
      "finished t20\n",
      "done and done\n"
     ]
    }
   ],
   "source": [
    "#this code generates a list of urls, urls for pages on each of the grounds, this page will have the country information\n",
    "#that we will scrape later. \n",
    "\n",
    "from lxml import html\n",
    "import csv\n",
    "import os\n",
    "\n",
    "headings = ['ground','ground_url']\n",
    "with open(\"ground_urls.csv\", \"w\") as filem:   \n",
    "    wr = csv.writer(filem, delimiter = ',' , quotechar = '\"' )\n",
    "    wr.writerow(headings)\n",
    "\n",
    "for i in [\"test\",\"odi\",\"t20\"]:\n",
    "    d = 0\n",
    "    print \"starting\" + i\n",
    "    for j in os.listdir(i):\n",
    "        filex = i + \"/\" + j\n",
    "        print \"processing \" + filex\n",
    "        filey =  os.path.join(os.path.dirname(os.path.realpath('__file__')), filex)\n",
    "        with open(filey) as filez:\n",
    "            sourcex = filez.read()\n",
    "            treex = html.document_fromstring(sourcex)\n",
    "            nodes = treex.xpath(\"//td[@valign='top']/a[contains(@href,'/Scorecards/')]\")\n",
    "            for node in nodes:\n",
    "                row_list = []\n",
    "                ground = node.xpath(\"following::td[1]/a/text()\")[0]\n",
    "                row_list.append(ground)\n",
    "                ground_url = node.xpath(\"following::td[1]/a/@href\")[0]\n",
    "                row_list.append(ground_url)\n",
    "                with open(\"ground_urls.csv\", \"a\") as filen:\n",
    "                    wrz = csv.writer(filen, delimiter = ',' , quotechar = '\"' )\n",
    "                    wrz.writerow(row_list)\n",
    "        print \"finished \" + filex\n",
    "    print \"finished \" + i\n",
    "\n",
    "filen.close()\n",
    "print 'done and done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('ground_urls.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ground</th>\n",
       "      <th>ground_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Edgbaston, Birmingham</td>\n",
       "      <td>/Archive/Grounds/11/321.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lord's Cricket Ground, St John's Wood</td>\n",
       "      <td>/Archive/Grounds/11/596.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Old Trafford, Manchester</td>\n",
       "      <td>/Archive/Grounds/11/618.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Headingley, Leeds</td>\n",
       "      <td>/Archive/Grounds/11/570.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R Premadasa Stadium, Colombo</td>\n",
       "      <td>/Archive/Grounds/25/1707.html</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ground                     ground_url\n",
       "0                  Edgbaston, Birmingham   /Archive/Grounds/11/321.html\n",
       "1  Lord's Cricket Ground, St John's Wood   /Archive/Grounds/11/596.html\n",
       "2               Old Trafford, Manchester   /Archive/Grounds/11/618.html\n",
       "3                      Headingley, Leeds   /Archive/Grounds/11/570.html\n",
       "4           R Premadasa Stadium, Colombo  /Archive/Grounds/25/1707.html"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['complete_url'] = \"http://cricketarchive.com\" + df['ground_url'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://cricketarchive.com/Archive/Grounds/11/596.html'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['complete_url'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ground</th>\n",
       "      <th>ground_url</th>\n",
       "      <th>complete_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Edgbaston, Birmingham</td>\n",
       "      <td>/Archive/Grounds/11/321.html</td>\n",
       "      <td>http://cricketarchive.com/Archive/Grounds/11/3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lord's Cricket Ground, St John's Wood</td>\n",
       "      <td>/Archive/Grounds/11/596.html</td>\n",
       "      <td>http://cricketarchive.com/Archive/Grounds/11/5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Old Trafford, Manchester</td>\n",
       "      <td>/Archive/Grounds/11/618.html</td>\n",
       "      <td>http://cricketarchive.com/Archive/Grounds/11/6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Headingley, Leeds</td>\n",
       "      <td>/Archive/Grounds/11/570.html</td>\n",
       "      <td>http://cricketarchive.com/Archive/Grounds/11/5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R Premadasa Stadium, Colombo</td>\n",
       "      <td>/Archive/Grounds/25/1707.html</td>\n",
       "      <td>http://cricketarchive.com/Archive/Grounds/25/1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ground                     ground_url  \\\n",
       "0                  Edgbaston, Birmingham   /Archive/Grounds/11/321.html   \n",
       "1  Lord's Cricket Ground, St John's Wood   /Archive/Grounds/11/596.html   \n",
       "2               Old Trafford, Manchester   /Archive/Grounds/11/618.html   \n",
       "3                      Headingley, Leeds   /Archive/Grounds/11/570.html   \n",
       "4           R Premadasa Stadium, Colombo  /Archive/Grounds/25/1707.html   \n",
       "\n",
       "                                        complete_url  \n",
       "0  http://cricketarchive.com/Archive/Grounds/11/3...  \n",
       "1  http://cricketarchive.com/Archive/Grounds/11/5...  \n",
       "2  http://cricketarchive.com/Archive/Grounds/11/6...  \n",
       "3  http://cricketarchive.com/Archive/Grounds/11/5...  \n",
       "4  http://cricketarchive.com/Archive/Grounds/25/1...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['http://cricketarchive.com/Archive/Grounds/11/321.html',\n",
       "       'http://cricketarchive.com/Archive/Grounds/11/596.html',\n",
       "       'http://cricketarchive.com/Archive/Grounds/11/618.html',\n",
       "       'http://cricketarchive.com/Archive/Grounds/11/570.html',\n",
       "       'http://cricketarchive.com/Archive/Grounds/25/1707.html',\n",
       "       'http://cricketarchive.com/Archive/Grounds/11/664.html',\n",
       "       'http://cricketarchive.com/Archive/Grounds/25/1708.html',\n",
       "       'http://cricketarchive.com/Archive/Grounds/11/594.html',\n",
       "       'http://cricketarchive.com/Archive/Grounds/11/407.html',\n",
       "       'http://cricketarchive.com/Archive/Grounds/25/1719.html',\n",
       ".\n",
       ".\n",
       ".\n",
       "       'http://cricketarchive.com/Archive/Grounds/19/1344.html',\n",
       "       'http://cricketarchive.com/Archive/Grounds/60/1297.html',\n",
       "       'http://cricketarchive.com/Archive/Grounds/2/83.html'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_urls = df['complete_url'].unique()\n",
    "new_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://cricketarchive.com/Archive/Grounds/11/321.html',\n",
       " 'http://cricketarchive.com/Archive/Grounds/11/596.html',\n",
       " 'http://cricketarchive.com/Archive/Grounds/11/618.html',\n",
       ".\n",
       ".\n",
       ".\n",
       " 'http://cricketarchive.com/Archive/Grounds/4/246.html',\n",
       " 'http://cricketarchive.com/Archive/Grounds/19/1344.html',\n",
       " 'http://cricketarchive.com/Archive/Grounds/60/1297.html',\n",
       " 'http://cricketarchive.com/Archive/Grounds/2/83.html']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the array is just changed to a list, python terms, don't worry about it\n",
    "\n",
    "new_list = new_urls.tolist()\n",
    "new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18dd971b52950cc851836d0ed276001c\n"
     ]
    }
   ],
   "source": [
    "#this logs in to the website again\n",
    "#the list of urls we've generated for each of the grounds will now be opened and the pages downloaded\n",
    "\n",
    "import requests\n",
    "import re\n",
    "\n",
    "session_requests = requests.session()\n",
    "login_url = \"https://my.cricketarchive.com/\"\n",
    "result = session_requests.get(login_url)\n",
    "sourcez = result.text\n",
    "\n",
    "#most of the code below taken from https://kazuar.github.io/scraping-tutorial/ and\n",
    "#https://brennan.io/2016/03/02/logging-in-with-requests/\n",
    "\n",
    "csrf_hash2 =str((re.findall(r\"var csrfHash = '(.*)';\", sourcez))[0]) \n",
    "\n",
    "f = open('passwords_ignore.txt',\"r\")\n",
    "lines = f.readlines()\n",
    "emailid = lines[0].strip()\n",
    "passwd = lines[1].strip()\n",
    "\n",
    "payload = {\"email\": emailid,\"password\": passwd,\"vo-action\": \"login\",\"csrfHash\": csrf_hash2}\n",
    "\n",
    "print csrf_hash2\n",
    "\n",
    "response = session_requests.post(login_url,data = payload,headers = dict(referer=login_url))\n",
    "\n",
    "#r = session_requests.get('http://cricketarchive.com/Archive/Seasons/Lists/2016_itt_Match_List.html')\n",
    "#print r.text\n",
    "#this is to test if we've logged in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs('grounds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "England\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['England']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is just test code to see if we can scrape the country field from the page for each ground\n",
    "\n",
    "from lxml import html\n",
    "import os\n",
    "\n",
    "country_list = []\n",
    "filenamey = 'grounds/001.html'\n",
    "fnamez = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenamey)\n",
    "with open(fnamez) as filez:\n",
    "    sourcex = filez.read()\n",
    "#print sourcex\n",
    "treex = html.document_fromstring(sourcex)\n",
    "#print(html.tostring(treex, pretty_print=True))\n",
    "country = treex.xpath('//b[text()=\"Country:\"]/following::td[1]/text()')[0]\n",
    "#country = treex.xpath('//*[@id=\"columnLeft\"]/table/tbody/tr[3]/td[2]/text()')[0]\n",
    "print country\n",
    "country_list.append(country)\n",
    "#print \"finished \" + urlx + \", added\" + country\n",
    "country_list\n",
    "#print 'finished making country_list'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing http://cricketarchive.com/Archive/Grounds/11/321.html\n",
      "England\n",
      "finished http://cricketarchive.com/Archive/Grounds/11/321.html, added England\n",
      "processing http://cricketarchive.com/Archive/Grounds/11/596.html\n",
      "England\n",
      "finished http://cricketarchive.com/Archive/Grounds/11/596.html, added England\n",
      "processing http://cricketarchive.com/Archive/Grounds/11/618.html\n",
      "England\n",
      "finished http://cricketarchive.com/Archive/Grounds/11/618.html, added England\n",
      ".\n",
      ".\n",
      ".\n",
      "processing http://cricketarchive.com/Archive/Grounds/60/1297.html\n",
      "Ireland\n",
      "finished http://cricketarchive.com/Archive/Grounds/60/1297.html, added Ireland\n",
      "processing http://cricketarchive.com/Archive/Grounds/2/83.html\n",
      "Australia\n",
      "finished http://cricketarchive.com/Archive/Grounds/2/83.html, added Australia\n",
      "finished making country_list\n"
     ]
    }
   ],
   "source": [
    "#this generates a list of countries, the list is in the order of the urls that we'd generated of the grounds\n",
    "#the fact that they're in the same order will prove useful later, as we'll be merging this list with that file\n",
    "#or 'concatenating' which is the technical term used to describe the process\n",
    "\n",
    "\n",
    "import time\n",
    "from lxml import html\n",
    "from lxml.cssselect import CSSSelector\n",
    "from random import randint\n",
    "\n",
    "country_list = []\n",
    "#d = 0\n",
    "for urlx in new_list:\n",
    "    print \"processing \" + urlx\n",
    "    responseq = session_requests.get(urlx)\n",
    "    #d += 1\n",
    "    #zrs = str(d).zfill(3)\n",
    "    #number = zrs + '.html'\n",
    "    #filenamey = 'grounds/' + number\n",
    "    #fnamez = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenamey)\n",
    "    #with open(fnamez, \"w\") as filex:\n",
    "     #   filex.write(responseq.content)\n",
    "    #  filex.close()\n",
    "    #print \"written \" + urlx\n",
    "    #with open(fnamez) as filez:\n",
    "    #    sourcex = filez.read()\n",
    "    #treex = html.document_fromstring(sourcex)\n",
    "    #treex = html.document_fromstring(responseq.text)\n",
    "    #treex = html.document_fromstring(responseq.content)\n",
    "    treex = html.document_fromstring(responseq.text)\n",
    "    #country = treex.xpath('//*[@id=\"columnLeft\"]/table/tbody/tr[3]/td[2]/text()')[0]\n",
    "    country = treex.xpath('//b[text()=\"Country:\"]/following::td[1]/text()')[0]\n",
    "    print country\n",
    "    country_list.append(country)\n",
    "    print \"finished \" + urlx + \", added \" + country\n",
    "    delay = randint(10,15)\n",
    "    time.sleep(delay)\n",
    "\n",
    "print 'finished making country_list'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['England', 'England', 'England', 'England', 'Sri Lanka', 'England', 'Sri Lanka', 'England', 'England', 'Sri Lanka', 'Pakistan', 'Pakistan', 'Pakistan', 'Australia', 'India', 'Australia', 'Australia', 'Australia', 'New Zealand', 'Australia', 'New Zealand', 'New Zealand', 'West Indies', 'West Indies', 'West Indies', 'West Indies', 'West Indies', 'Pakistan', 'Pakistan', 'New Zealand', 'Zimbabwe', 'Zimbabwe', 'South Africa', 'South Africa', 'Sri Lanka', 'South Africa', 'South Africa', 'India', 'India', 'India', 'India', 'Australia', 'Pakistan', 'Pakistan', 'India', 'India', 'India', 'Sri Lanka', 'Zimbabwe', 'India', 'India', 'New Zealand', 'New Zealand', 'Pakistan', 'India', 'South Africa', 'Pakistan', 'India', 'West Indies', 'Bangladesh', 'Sri Lanka', 'South Africa', 'Pakistan', 'Bangladesh', 'United Arab Emirates', 'South Africa', 'South Africa', 'West Indies', 'West Indies', 'Australia', 'Australia', 'Bangladesh', 'Bangladesh', 'Bangladesh', 'West Indies', 'New Zealand', 'Bangladesh', 'West Indies', 'India', 'West Indies', 'India', 'Wales', 'India', 'United Arab Emirates', 'United Arab Emirates', 'Sri Lanka', 'West Indies', 'England', 'Bangladesh', 'New Zealand', 'India', 'India', 'India', 'India', 'Canada', 'England', 'England', 'Sri Lanka', 'Pakistan', 'India', 'India', 'India', 'India', 'Pakistan', 'Pakistan', 'New Zealand', 'Australia', 'Australia', 'Australia', 'Australia', 'Australia', 'India', 'India', 'India', 'India', 'India', 'India', 'India', 'India', 'India', 'India', 'India', 'India', 'Singapore', 'Kenya', 'Kenya', 'Kenya', 'Pakistan', 'South Africa', 'South Africa', 'India', 'South Africa', 'New Zealand', 'New Zealand', 'England', 'England', 'England', 'England', 'England', 'England', 'Ireland', 'England', 'Scotland', 'Netherlands', 'England', 'Singapore', 'India', 'Australia', 'Kenya', 'India', 'Zimbabwe', 'New Zealand', 'South Africa', 'Morocco', 'Malaysia', 'Kenya', 'Kenya', 'Kenya', 'Ireland', 'Scotland', 'Scotland', 'Netherlands', 'Canada', 'Scotland', 'India', 'Sri Lanka', 'Netherlands', 'Netherlands', 'New Zealand', 'Netherlands', 'India', 'India', 'United Arab Emirates', 'New Zealand', 'New Zealand', 'New Zealand', 'Ireland', 'Australia', 'India', 'Malaysia', 'Hong Kong', 'India', 'South Africa', 'United States of America', 'Australia', 'Bangladesh', 'Namibia', 'Ireland', 'Australia']\n"
     ]
    }
   ],
   "source": [
    "print country_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(country_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_list == new_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://cricketarchive.com/Archive/Grounds/11/321.html',\n",
       " 'http://cricketarchive.com/Archive/Grounds/11/596.html',\n",
       " 'http://cricketarchive.com/Archive/Grounds/11/618.html',\n",
       ".\n",
       ".\n",
       ".\n",
       " 'http://cricketarchive.com/Archive/Grounds/11/570.html',\n",
       " 'http://cricketarchive.com/Archive/Grounds/19/1344.html',\n",
       " 'http://cricketarchive.com/Archive/Grounds/60/1297.html',\n",
       " 'http://cricketarchive.com/Archive/Grounds/2/83.html']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Archive/Grounds/11/321.html',\n",
       " '/Archive/Grounds/11/596.html',\n",
       " '/Archive/Grounds/11/618.html',\n",
       ".\n",
       ".\n",
       ".\n",
       " '/Archive/Grounds/11/570.html',\n",
       " '/Archive/Grounds/25/1707.html',\n",
       " '/Archive/Grounds/19/1344.html',\n",
       " '/Archive/Grounds/60/1297.html',\n",
       " '/Archive/Grounds/2/83.html']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ground_url</th>\n",
       "      <th>ground_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Archive/Grounds/11/321.html</td>\n",
       "      <td>England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Archive/Grounds/11/596.html</td>\n",
       "      <td>England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Archive/Grounds/11/618.html</td>\n",
       "      <td>England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Archive/Grounds/11/570.html</td>\n",
       "      <td>England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Archive/Grounds/25/1707.html</td>\n",
       "      <td>Sri Lanka</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      ground_url ground_country\n",
       "0   /Archive/Grounds/11/321.html        England\n",
       "1   /Archive/Grounds/11/596.html        England\n",
       "2   /Archive/Grounds/11/618.html        England\n",
       "3   /Archive/Grounds/11/570.html        England\n",
       "4  /Archive/Grounds/25/1707.html      Sri Lanka"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making a dataframe\n",
    "import pandas as pd\n",
    "\n",
    "combined = [('ground_url', new_list2),('ground_country', country_list)]\n",
    "df_new = pd.DataFrame.from_items(combined)\n",
    "df_new.head(n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(189, 2)\n"
     ]
    }
   ],
   "source": [
    "print df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ground</th>\n",
       "      <th>ground_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Edgbaston, Birmingham</td>\n",
       "      <td>/Archive/Grounds/11/321.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lord's Cricket Ground, St John's Wood</td>\n",
       "      <td>/Archive/Grounds/11/596.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Old Trafford, Manchester</td>\n",
       "      <td>/Archive/Grounds/11/618.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Headingley, Leeds</td>\n",
       "      <td>/Archive/Grounds/11/570.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R Premadasa Stadium, Colombo</td>\n",
       "      <td>/Archive/Grounds/25/1707.html</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ground                     ground_url\n",
       "0                  Edgbaston, Birmingham   /Archive/Grounds/11/321.html\n",
       "1  Lord's Cricket Ground, St John's Wood   /Archive/Grounds/11/596.html\n",
       "2               Old Trafford, Manchester   /Archive/Grounds/11/618.html\n",
       "3                      Headingley, Leeds   /Archive/Grounds/11/570.html\n",
       "4           R Premadasa Stadium, Colombo  /Archive/Grounds/25/1707.html"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('ground_urls.csv')\n",
    "df.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5087, 2)\n"
     ]
    }
   ],
   "source": [
    "print df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ground</th>\n",
       "      <th>ground_url</th>\n",
       "      <th>ground_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Edgbaston, Birmingham</td>\n",
       "      <td>/Archive/Grounds/11/321.html</td>\n",
       "      <td>England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lord's Cricket Ground, St John's Wood</td>\n",
       "      <td>/Archive/Grounds/11/596.html</td>\n",
       "      <td>England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Old Trafford, Manchester</td>\n",
       "      <td>/Archive/Grounds/11/618.html</td>\n",
       "      <td>England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Headingley, Leeds</td>\n",
       "      <td>/Archive/Grounds/11/570.html</td>\n",
       "      <td>England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R Premadasa Stadium, Colombo</td>\n",
       "      <td>/Archive/Grounds/25/1707.html</td>\n",
       "      <td>Sri Lanka</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ground                     ground_url  \\\n",
       "0                  Edgbaston, Birmingham   /Archive/Grounds/11/321.html   \n",
       "1  Lord's Cricket Ground, St John's Wood   /Archive/Grounds/11/596.html   \n",
       "2               Old Trafford, Manchester   /Archive/Grounds/11/618.html   \n",
       "3                      Headingley, Leeds   /Archive/Grounds/11/570.html   \n",
       "4           R Premadasa Stadium, Colombo  /Archive/Grounds/25/1707.html   \n",
       "\n",
       "  ground_country  \n",
       "0        England  \n",
       "1        England  \n",
       "2        England  \n",
       "3        England  \n",
       "4      Sri Lanka  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if I remember correctly this 'merging' actually does a kind of a vlookup thing if you're familiar with that function\n",
    "#from excel, so a new column 'ground_country' is created containing the right country for each ground\n",
    "\n",
    "df3 = df.merge(df_new, on='ground_url', how='left')\n",
    "df3.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5087, 3)\n"
     ]
    }
   ],
   "source": [
    "print df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ground_url</th>\n",
       "      <th>ground_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Archive/Grounds/11/321.html</td>\n",
       "      <td>England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Archive/Grounds/11/596.html</td>\n",
       "      <td>England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Archive/Grounds/11/618.html</td>\n",
       "      <td>England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Archive/Grounds/11/570.html</td>\n",
       "      <td>England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Archive/Grounds/25/1707.html</td>\n",
       "      <td>Sri Lanka</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      ground_url ground_country\n",
       "0   /Archive/Grounds/11/321.html        England\n",
       "1   /Archive/Grounds/11/596.html        England\n",
       "2   /Archive/Grounds/11/618.html        England\n",
       "3   /Archive/Grounds/11/570.html        England\n",
       "4  /Archive/Grounds/25/1707.html      Sri Lanka"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4 = df3.drop('ground',1)\n",
    "df4.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5087, 2)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>format</th>\n",
       "      <th>number</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>team1</th>\n",
       "      <th>team2</th>\n",
       "      <th>ground</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>test_0001</td>\n",
       "      <td>05 Jun 1997</td>\n",
       "      <td>1997</td>\n",
       "      <td>England</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Edgbaston, Birmingham</td>\n",
       "      <td>/Archive/Scorecards/64/64098.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>test_0002</td>\n",
       "      <td>19 Jun 1997</td>\n",
       "      <td>1997</td>\n",
       "      <td>England</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Lord's Cricket Ground, St John's Wood</td>\n",
       "      <td>/Archive/Scorecards/64/64171.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>test_0003</td>\n",
       "      <td>03 Jul 1997</td>\n",
       "      <td>1997</td>\n",
       "      <td>England</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Old Trafford, Manchester</td>\n",
       "      <td>/Archive/Scorecards/64/64259.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test</td>\n",
       "      <td>test_0004</td>\n",
       "      <td>24 Jul 1997</td>\n",
       "      <td>1997</td>\n",
       "      <td>England</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Headingley, Leeds</td>\n",
       "      <td>/Archive/Scorecards/64/64378.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test</td>\n",
       "      <td>test_0005</td>\n",
       "      <td>02 Aug 1997</td>\n",
       "      <td>1997</td>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>India</td>\n",
       "      <td>R Premadasa Stadium, Colombo</td>\n",
       "      <td>/Archive/Scorecards/64/64422.html</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  format     number         date  year      team1      team2  \\\n",
       "0   test  test_0001  05 Jun 1997  1997    England  Australia   \n",
       "1   test  test_0002  19 Jun 1997  1997    England  Australia   \n",
       "2   test  test_0003  03 Jul 1997  1997    England  Australia   \n",
       "3   test  test_0004  24 Jul 1997  1997    England  Australia   \n",
       "4   test  test_0005  02 Aug 1997  1997  Sri Lanka      India   \n",
       "\n",
       "                                  ground                                url  \n",
       "0                  Edgbaston, Birmingham  /Archive/Scorecards/64/64098.html  \n",
       "1  Lord's Cricket Ground, St John's Wood  /Archive/Scorecards/64/64171.html  \n",
       "2               Old Trafford, Manchester  /Archive/Scorecards/64/64259.html  \n",
       "3                      Headingley, Leeds  /Archive/Scorecards/64/64378.html  \n",
       "4           R Premadasa Stadium, Colombo  /Archive/Scorecards/64/64422.html  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5 = pd.read_csv('url_store.csv')\n",
    "df5.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5087, 8)\n"
     ]
    }
   ],
   "source": [
    "print df5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>format</th>\n",
       "      <th>number</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>team1</th>\n",
       "      <th>team2</th>\n",
       "      <th>ground</th>\n",
       "      <th>url</th>\n",
       "      <th>ground_url</th>\n",
       "      <th>ground_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>test_0001</td>\n",
       "      <td>05 Jun 1997</td>\n",
       "      <td>1997</td>\n",
       "      <td>England</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Edgbaston, Birmingham</td>\n",
       "      <td>/Archive/Scorecards/64/64098.html</td>\n",
       "      <td>/Archive/Grounds/11/321.html</td>\n",
       "      <td>England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>test_0002</td>\n",
       "      <td>19 Jun 1997</td>\n",
       "      <td>1997</td>\n",
       "      <td>England</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Lord's Cricket Ground, St John's Wood</td>\n",
       "      <td>/Archive/Scorecards/64/64171.html</td>\n",
       "      <td>/Archive/Grounds/11/596.html</td>\n",
       "      <td>England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>test_0003</td>\n",
       "      <td>03 Jul 1997</td>\n",
       "      <td>1997</td>\n",
       "      <td>England</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Old Trafford, Manchester</td>\n",
       "      <td>/Archive/Scorecards/64/64259.html</td>\n",
       "      <td>/Archive/Grounds/11/618.html</td>\n",
       "      <td>England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test</td>\n",
       "      <td>test_0004</td>\n",
       "      <td>24 Jul 1997</td>\n",
       "      <td>1997</td>\n",
       "      <td>England</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Headingley, Leeds</td>\n",
       "      <td>/Archive/Scorecards/64/64378.html</td>\n",
       "      <td>/Archive/Grounds/11/570.html</td>\n",
       "      <td>England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test</td>\n",
       "      <td>test_0005</td>\n",
       "      <td>02 Aug 1997</td>\n",
       "      <td>1997</td>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>India</td>\n",
       "      <td>R Premadasa Stadium, Colombo</td>\n",
       "      <td>/Archive/Scorecards/64/64422.html</td>\n",
       "      <td>/Archive/Grounds/25/1707.html</td>\n",
       "      <td>Sri Lanka</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  format     number         date  year      team1      team2  \\\n",
       "0   test  test_0001  05 Jun 1997  1997    England  Australia   \n",
       "1   test  test_0002  19 Jun 1997  1997    England  Australia   \n",
       "2   test  test_0003  03 Jul 1997  1997    England  Australia   \n",
       "3   test  test_0004  24 Jul 1997  1997    England  Australia   \n",
       "4   test  test_0005  02 Aug 1997  1997  Sri Lanka      India   \n",
       "\n",
       "                                  ground                                url  \\\n",
       "0                  Edgbaston, Birmingham  /Archive/Scorecards/64/64098.html   \n",
       "1  Lord's Cricket Ground, St John's Wood  /Archive/Scorecards/64/64171.html   \n",
       "2               Old Trafford, Manchester  /Archive/Scorecards/64/64259.html   \n",
       "3                      Headingley, Leeds  /Archive/Scorecards/64/64378.html   \n",
       "4           R Premadasa Stadium, Colombo  /Archive/Scorecards/64/64422.html   \n",
       "\n",
       "                      ground_url ground_country  \n",
       "0   /Archive/Grounds/11/321.html        England  \n",
       "1   /Archive/Grounds/11/596.html        England  \n",
       "2   /Archive/Grounds/11/618.html        England  \n",
       "3   /Archive/Grounds/11/570.html        England  \n",
       "4  /Archive/Grounds/25/1707.html      Sri Lanka  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the dataframe below is the result of 'concatenating' which just means joining two things along a side or 'axis'\n",
    "#this dataframe will eventually become a csv that we use to extract the country that each ground is based in\n",
    "#dont think there was any other use of this file to be honest! everything else in the file could be extracted from the \n",
    "#scraping we're doing next\n",
    "\n",
    "df6 = pd.concat([df5, df4], axis=1)\n",
    "df6.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5087, 10)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df6.to_csv('url_store_complete.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5087, 10)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "reader = csv.DictReader(open('url_store_complete.csv'))\n",
    "\n",
    "gdict = {}\n",
    "for row in reader:\n",
    "    key = row.pop('number')\n",
    "    gdict[key] = row\n",
    "#print gdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ireland\n"
     ]
    }
   ],
   "source": [
    "#just testing out some code that will be part of some other code later on\n",
    "#if you're someone who uses python to scrape websites, look at the comment below, it'll help avoid a lot of grief\n",
    "\n",
    "\n",
    "from lxml import html\n",
    "import os\n",
    "\n",
    "filenamey = 'code_check/odi_1915.html'\n",
    "fnamez =  os.path.join(os.path.dirname(os.path.realpath('__file__')), filenamey)\n",
    "with open(fnamez) as filez:\n",
    "    sourcex = filez.read()\n",
    "treex = html.document_fromstring(sourcex)\n",
    "\n",
    "#print(html.tostring(treex, pretty_print=True))\n",
    "\n",
    "#was stuck on this for a day, seems that the xpath returned by firebug or chrome's inspector tools won't work\n",
    "#sometimes in python. According to this page http://stackoverflow.com/questions/32015083/python-xpath-returns-an-empty-list\n",
    "#the way your browser parses html and lxml parses html is different\n",
    "#for example, the xpath I used first to get a certain element was this:\n",
    "#'//*[@id=\"columnLeft\"]/table[1]/tbody/tr[1]/td[2]/b/font/center/a[1]/text()' . This was returned by Chrome but didn't work\n",
    "#here because python parsed the page differently. The xpath returned by the browser tools also introduced a tbody tag\n",
    "#which wasn't there in the original html. Ultimately used the code below.\n",
    "\n",
    "divx = treex.xpath(\"//*[@id='columnLeft']/table[1]/tr[1]/td[2]/center/a[1]/text()\")[0]\n",
    "\n",
    "print divx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing test_0001.html\n",
      "finished test_0001.html\n",
      "\n",
      "processing test_0002.html\n",
      "finished test_0002.html\n",
      "\n",
      ".\n",
      ".\n",
      ".\n",
      "processing t20_0616.html\n",
      "finished t20_0616.html\n",
      "\n",
      "processing t20_0617.html\n",
      "finished t20_0617.html\n",
      "\n",
      "Processing done\n"
     ]
    }
   ],
   "source": [
    "#this is the most important piece of code in this whole ipython notebook\n",
    "#what this does is go through the scoreboard for each t20, ODI and test match from the past 25 years and gets all the\n",
    "#information we will use later on. Everything from the umpire's name and who won the toss to traditional scoreboard data\n",
    "#such as the number of balls each batsman faced and the number of maidens overs bowled\n",
    "\n",
    "#at the end of this code block, three json files will be created, one each for the data on tests, ODIs and T20s\n",
    "#don't know if anyone non-techie is still looking at this, but json is a better format than csv when some \n",
    "#scoreboards contain more data than others, it accommodates varying levels of depth, if that makes sense\n",
    "\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "#import collections\n",
    "import json\n",
    "import os\n",
    "from lxml import html\n",
    "import re\n",
    "import io\n",
    "\n",
    "formats = [\"test\",\"odi\",\"t20\"]\n",
    "for formatx in formats:\n",
    "    \n",
    "    tree = lambda: defaultdict(tree)\n",
    "    jdict = tree()  #this ensures a fresh dictionary/json is created for each cricket format\n",
    "    #so the code above has been used to create a dynamic tree.Got the code from http://sopython.com/wiki/Fancy_Python_Tricks and \n",
    "    #https://gist.github.com/hrldcpr/2012250 , seems it's something called autovivification. Whatever.\n",
    "    \n",
    "    folderx = formatx + '_cards'\n",
    "    for i in os.listdir(folderx):\n",
    "        if i not in ['odi_2018.html']: #this is a troublesome file/scoreboard that I excluded\n",
    "            #loops through all the files in the folder\n",
    "            print \"processing \" + i\n",
    "            filex = formatx + '_cards/' + i\n",
    "            filey =  os.path.join(os.path.dirname(os.path.realpath('__file__')), filex)\n",
    "            with open(filey) as filez:\n",
    "                sourcex = filez.read()\n",
    "                treex = html.document_fromstring(sourcex)\n",
    "                #print(html.tostring(treex, pretty_print=True))\n",
    "                jdict[i]['format'] = (re.findall(r\"(.*)_.*\", i))[0] #whether test, odi, t20\n",
    "                jdict[i]['team1'] = treex.xpath('//*[@id=\"columnLeft\"]/table[1]/tr[1]/td[2]/center/a[1]/text()')[0]\n",
    "                jdict[i]['team2'] = treex.xpath('//*[@id=\"columnLeft\"]/table[1]/tr[1]/td[2]/center/a[2]/text()')[0]\n",
    "                jdict[i]['tour'] = treex.xpath('//*[@id=\"columnLeft\"]/table[1]/tr[2]/td[2]/center/a/text()')[0]\n",
    "\n",
    "                jdict[i]['venue_text']= ''.join(treex.xpath('//*[@id=\"columnLeft\"]/table[1]//tr//td[contains(text(),\"Venue\")]\\\n",
    "                /following-sibling::td[1]//text()'))\n",
    "                #print jdict[i]['venue_text']\n",
    "                #you can find out if a match is day-night or not by doing a search for that substring against venue_text\n",
    "\n",
    "                jdict[i]['year']= (re.findall(r'\\d{4}',jdict[i]['venue_text']))[0]\n",
    "                jdict[i]['venue']= treex.xpath('//*[@id=\"columnLeft\"]/table[1]//tr//td[contains(text(),\"Venue\")]/\\\n",
    "                                    following-sibling::td[1]/a/text()')[0]\n",
    "\n",
    "                #details below are from gdict, which is url_store_complete.csv in a dictionary, gets us countries of grounds\n",
    "                j = (re.findall(r'(.*).html', i))[0]\n",
    "                jdict[i]['start_date']= gdict[j]['date']\n",
    "                jdict[i]['venue_country']= gdict[j]['ground_country']\n",
    "                jdict[i]['venue_url']= gdict[j]['ground_url']\n",
    "                jdict[i]['scoreboard_url']= gdict[j]['url']\n",
    "                try:\n",
    "                    jdict[i]['toss_text']= treex.xpath('//*[@id=\"columnLeft\"]/table[1]//tr//td[contains(text(),\"Toss\")]\\\n",
    "                    /following-sibling::td[1]/text()')[0]\n",
    "                    jdict[i]['toss_winner']= (re.findall(r'(.*) won .*',jdict[i]['toss_text']))[0]\n",
    "                except:\n",
    "                    pass\n",
    "                else:\n",
    "                    jdict[i]['toss_winner_choice']= (re.findall(r'.* decided to (.*)',jdict[i]['toss_text']))[0]\n",
    "                jdict[i]['result_text']= ''.join(treex.xpath('//*[@id=\"columnLeft\"]/table[1]//tr//td[contains(text(),\"Result\")]\\\n",
    "                /following-sibling::td[1]//text()'))\n",
    "                #print jdict[i]['result_text']\n",
    "                if ' won by ' in jdict[i]['result_text']:\n",
    "                    jdict[i]['winner']= (re.findall(r'(.*) won by .*',jdict[i]['result_text']))[0]\n",
    "                    jdict[i]['winning_margin']= (re.findall(r'.* won by (.*)',jdict[i]['result_text']))[0]\n",
    "\n",
    "                    #if you want to find out draws in tests, just search for draw against result_text\n",
    "                    #do a word cloud of result text to see what other outcomes are possible - cancelled, tied, suspended etc.\n",
    "                    #also if you're checking for how many matches ended in actual wins, search for \" won by \", not the word\n",
    "                    #'won', because that might bring in results for t20 matches decided in super-overs, where there's no winning margin\n",
    "                try:\n",
    "                    jdict[i]['umpires_text'] = ''.join(treex.xpath('//*[@id=\"columnLeft\"]/table[1]//tr//td[contains(text(),\"Umpires\")]\\\n",
    "                    /following-sibling::td[1]//text()'))\n",
    "                    #print jdict[i]['umpires_text']\n",
    "                except:\n",
    "                    pass\n",
    "                else:\n",
    "                    jdict[i]['umpires_names'] = treex.xpath('//*[@id=\"columnLeft\"]/table[1]//tr//td[contains(text(),\"Umpires\")]\\\n",
    "                    /following-sibling::td[1]//a/text()')\n",
    "                    jdict[i]['umpires_urls'] = treex.xpath('//*[@id=\"columnLeft\"]/table[1]//tr//td[contains(text(),\"Umpires\")]\\\n",
    "                    /following-sibling::td[1]//a/@href')\n",
    "                try:\n",
    "                    jdict[i]['man_match']= treex.xpath('//*[@id=\"columnLeft\"]/table[1]//tr//td[contains(text(),\"Man of the Match\")]\\\n",
    "                    /following-sibling::td[1]/a/text()')[0]\n",
    "                    jdict[i]['man_match_url']= treex.xpath('//*[@id=\"columnLeft\"]/table[1]//tr//td[contains(text(),\"Man of the Match\")]\\\n",
    "                    /following-sibling::td[1]/a/@href')[0]\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                #now to get the batting and bowling stats\n",
    "\n",
    "                innings = treex.xpath('//b[text()[contains(.,\"innings\") and not(contains(.,\"super\"))]]')\n",
    "                #got the code above from http://stackoverflow.com/questions/3655549\n",
    "                #/xpath-containstext-some-string-doesnt-work-when-used-with-node-with-more\n",
    "                #added the not contains 'super' to exclude super-over innings from t20 matches\n",
    "                #print innings (this will give a list of <b> or bold nodes)\n",
    "\n",
    "                order =0\n",
    "\n",
    "                for inning in innings:\n",
    "                    #inning_name = ''.join([inning.xpath('a/text()')[0],inning.xpath('text()')[0]]) this didn't work\n",
    "                    #inning_name = inning.text_content().lstrip(' ')\n",
    "                    order += 1\n",
    "                    inning_order = order\n",
    "\n",
    "                    #print inning_name\n",
    "                    #print type(inning_name)\n",
    "                    team_batting = inning.xpath('a/text()')[0]\n",
    "                    #print team_batting\n",
    "\n",
    "                    #CHECKS\n",
    "                    if inning.xpath('following::td[1]/b/text()')[0] != 'Runs':\n",
    "                        print \"Runs figures wrong\"\n",
    "                    if inning.xpath('following::td[2]/b/text()')[0] != 'Balls':\n",
    "                        print \"Balls figures wrong\"\n",
    "                    if inning.xpath('following::td[4]/b/text()')[0] != '4s':\n",
    "                        print \"4s figures wrong\"\n",
    "                    if inning.xpath('following::td[5]/b/text()')[0] != '6s':\n",
    "                        print \"6s figures wrong\"\n",
    "\n",
    "\n",
    "                    batsman_list = inning.xpath('following::tr[position() >= 1 and not(position() > 11)]/td[1]')\n",
    "                    #print batsman_list\n",
    "                    d = 0\n",
    "                    for batsman in batsman_list:\n",
    "                        d += 1\n",
    "                        batsman_name = batsman.xpath('a/text()')[0]\n",
    "                        #print batsman_name\n",
    "                        batsman_order = d\n",
    "\n",
    "                        jdict[i]['innings'][inning_order]['team_batting'][team_batting]['batsman_all'][batsman_order]\\\n",
    "                            [batsman_name]['batsman_url'] = batsman.xpath('a/@href')[0]\n",
    "\n",
    "                        jdict[i]['innings'][inning_order]['team_batting'][team_batting]['batsman_all'][batsman_order]\\\n",
    "                            [batsman_name]['batsman_dismissal_text'] = ''.join(batsman.xpath('following::td[1]//text()'))\n",
    "                        jdict[i]['innings'][inning_order]['team_batting'][team_batting]['batsman_all'][batsman_order]\\\n",
    "                            [batsman_name]['batsman_dismissal_unlinked'] = ' '.join(batsman.xpath('following::td[1]/text()'))\n",
    "                        jdict[i]['innings'][inning_order]['team_batting'][team_batting]['batsman_all'][batsman_order]\\\n",
    "                            [batsman_name]['batsman_dismissal_names'] = batsman.xpath('following::td[1]//a/text()')\n",
    "                        jdict[i]['innings'][inning_order]['team_batting'][team_batting]['batsman_all'][batsman_order]\\\n",
    "                            [batsman_name]['batsman_dismissal_urls'] = batsman.xpath('following::td[1]//a/@href') \n",
    "\n",
    "                        if 'absent hurt' in ''.join(batsman.xpath('following::td[1]//text()')):\n",
    "                            pass\n",
    "                        else:\n",
    "                            jdict[i]['innings'][inning_order]['team_batting'][team_batting]['batsman_all'][batsman_order]\\\n",
    "                                [batsman_name]['batsman_runs'] = batsman.xpath('following::td[2]/text()')[0]\n",
    "                            jdict[i]['innings'][inning_order]['team_batting'][team_batting]['batsman_all'][batsman_order]\\\n",
    "                                [batsman_name]['batsman_balls'] = batsman.xpath('following::td[3]/text()')[0]\n",
    "                            jdict[i]['innings'][inning_order]['team_batting'][team_batting]['batsman_all'][batsman_order]\\\n",
    "                                [batsman_name]['batsman_4s'] = batsman.xpath('following::td[5]/text()')[0]\n",
    "                            jdict[i]['innings'][inning_order]['team_batting'][team_batting]['batsman_all'][batsman_order]\\\n",
    "                                [batsman_name]['batsman_6s'] = batsman.xpath('following::td[6]/text()')[0]\n",
    "\n",
    "\n",
    "                        #if you want to find out if someone is not out, just search for substring 'not out' against dismissal_unlinked\n",
    "                        #if you want to find if someone is stumped, search for 'st' etc.\n",
    "                        #do a word cloud to find out various methods of dismissal\n",
    "                        #word cloud will also have \"did not bat\" \"retired hurt\" etc.\n",
    "                        #in dismissal text, see if number of run outs are rising, is that a sign of poor batsmanship?\n",
    "                        #of how much more frenzied things are getting in forms of cricket other than t20?\n",
    "                        #just do a search for 'run out' substring in batsman_dismissal_text\n",
    "\n",
    "\n",
    "                    try:\n",
    "                        jdict[i]['innings'][inning_order]['team_batting'][team_batting]['extras']['text'] = \\\n",
    "                                inning.xpath('following::tr[12]/td[2]/text()')[0]\n",
    "                    except:\n",
    "                        pass\n",
    "                    else:\n",
    "                        #if int(inning.xpath('following::tr[12]/td[3]/text()')[0]) == 0:\n",
    "                        jdict[i]['innings'][inning_order]['team_batting'][team_batting]['extras']['runs'] = \\\n",
    "                                inning.xpath('following::tr[12]/td[3]/text()')[0]\n",
    "                        extras_types = \\\n",
    "                             re.findall(r\"[a-z]+\", jdict[i]['innings'][inning_order]['team_batting'][team_batting]['extras']['text'])\n",
    "                        extras_figures = \\\n",
    "                             re.findall(r\"[0-9]+\", jdict[i]['innings'][inning_order]['team_batting'][team_batting]['extras']['text'])\n",
    "                        extras_dict = dict(zip(extras_types, extras_figures))\n",
    "                        jdict[i]['innings'][inning_order]['team_batting'][team_batting]['extras']['details'] = extras_dict\n",
    "\n",
    "                    #the details above give you number of wides, legbyes, byes etc.\n",
    "\n",
    "\n",
    "                    try:\n",
    "                        jdict[i]['innings'][inning_order]['team_batting'][team_batting]['total']['text'] =\\\n",
    "                        inning.xpath('following::tr[13]/td[2]/text()')[0]\n",
    "                        jdict[i]['innings'][inning_order]['team_batting'][team_batting]['total']['overs'] = \\\n",
    "                            (re.findall(r\".*, (.*) overs\\)\", jdict[i]['innings'][inning_order]['team_batting'][team_batting]['total']['text']))[0]\n",
    "                    except:\n",
    "                        pass\n",
    "                    else:\n",
    "                        jdict[i]['innings'][inning_order]['team_batting'][team_batting]['total']['runs'] =\\\n",
    "                            inning.xpath('following::tr[13]/td[3]/text()')[0]\n",
    "                        if 'all out' in jdict[i]['innings'][inning_order]['team_batting'][team_batting]['total']['text']:\n",
    "                            jdict[i]['innings'][inning_order]['team_batting'][team_batting]['total']['wickets'] = '10'\n",
    "                                                                                        #dont put 'all out', keep all column\n",
    "                                                                                           #values as single type\n",
    "                        if 'wickets' in jdict[i]['innings'][inning_order]['team_batting'][team_batting]['total']['text']:\n",
    "                            jdict[i]['innings'][inning_order]['team_batting'][team_batting]['total']['wickets'] =\\\n",
    "                            (re.findall(r\"\\((.*) wickets, .*\", jdict[i]['innings'][inning_order]['team_batting'][team_batting]['total']['text']))[0]\n",
    "                        if '1 wicket, ' in jdict[i]['innings'][inning_order]['team_batting'][team_batting]['total']['text']:\n",
    "                            jdict[i]['innings'][inning_order]['team_batting'][team_batting]['total']['wickets'] = '1'\n",
    "                        \n",
    "                        if 'no wicket'in jdict[i]['innings'][inning_order]['team_batting'][team_batting]['total']['text']:\n",
    "                            jdict[i]['innings'][inning_order]['team_batting'][team_batting]['total']['wickets'] = '0'\n",
    "\n",
    "                    #note that sometimes teams declare too, you can check for that against the total text\n",
    "\n",
    "                    batsman_dismissed_names = inning.xpath('following::tr[15]/td[1]//a/text()')\n",
    "                    batsman_dismissed_urls = inning.xpath('following::tr[15]/td[1]//@href')\n",
    "                    #keep it as batsman throughout, don't make it batsmen just so it's gramatically correct\n",
    "                    #dont trip by using batsman in some places and batsmen in other places\n",
    "\n",
    "                    try:\n",
    "                        fall_text = ''.join(inning.xpath('following::tr[15]/td[1]//text()'))\n",
    "                        jdict[i]['innings'][inning_order]['team_batting'][team_batting]['fall_wickets']['text'] = fall_text\n",
    "                        fall_list = fall_text.split(\"), \")\n",
    "                        throwaway = (re.findall(r\"([0-9]+)-.*\", fall_list[0]))[0]\n",
    "                    except:\n",
    "                        pass\n",
    "                    else:\n",
    "                        for fall in fall_list:\n",
    "                            f = (re.findall(r\"([0-9]+)-.*\", fall))[0]\n",
    "                            #print f\n",
    "                            jdict[i]['innings'][inning_order]['team_batting'][team_batting]['fall_wickets'][f]['score']=\\\n",
    "                                (re.findall(r\".*-(.*) \\(.*\", fall))[0]\n",
    "                            try:\n",
    "                                jdict[i]['innings'][inning_order]['team_batting'][team_batting]['fall_wickets'][f]['over']=\\\n",
    "                                (re.findall(r\".*, (.*) o.*\", fall))[0]\n",
    "                            except:\n",
    "                                pass\n",
    "                            #jdict[i][inning_name][team_batting]['fall_wickets'][f]['name']=(re.findall(r\".* \\((.*), .*\", fall))[0]\n",
    "                            jdict[i]['innings'][inning_order]['team_batting'][team_batting]['fall_wickets'][f]['name']=\\\n",
    "                                batsman_dismissed_names[int(f)-1]\n",
    "                            jdict[i]['innings'][inning_order]['team_batting'][team_batting]['fall_wickets'][f]['url']=\\\n",
    "                                batsman_dismissed_urls[int(f)-1]\n",
    "\n",
    "                    #bowling figures\n",
    "\n",
    "                    try:\n",
    "                        team_bowling = inning.xpath('following::table[1]/tr[1]/td[1]/b/a/text()')[0]\n",
    "                    except:\n",
    "                        pass\n",
    "                    else:\n",
    "                        #CHECK if column labels are where we expect them to be\n",
    "                        if inning.xpath('following::table[1]/tr[1]/td[2]/b/text()')[0] != 'Overs':\n",
    "                            print \"Overs figures wrong\"\n",
    "                        if inning.xpath('following::table[1]/tr[1]/td[3]/b/text()')[0] != 'Mdns':\n",
    "                            print \"Maidens figures wrong\"\n",
    "                        if inning.xpath('following::table[1]/tr[1]/td[4]/b/text()')[0] != 'Runs':\n",
    "                            print \"Runs figures wrong\"\n",
    "                        if inning.xpath('following::table[1]/tr[1]/td[5]/b/text()')[0] != 'Wkts':\n",
    "                            print \"Wickets figures wrong\"\n",
    "\n",
    "                        bowler_rows = inning.xpath('following::table[1]//tr')\n",
    "                        #print bowler_rows\n",
    "\n",
    "                        for row in bowler_rows[1:-1]:\n",
    "                            bowler_name = row.xpath('td[1]/a/text()')[0]\n",
    "                            jdict[i]['innings'][inning_order]['team_bowling'][team_bowling]['bowlers_all'][bowler_name]['bowler_url']=\\\n",
    "                                row.xpath('td[1]/a/@href')[0]\n",
    "                            overs = row.xpath('td[2]/text()')[0]\n",
    "                            jdict[i]['innings'][inning_order]['team_bowling'][team_bowling]['bowlers_all'][bowler_name]['overs']= \\\n",
    "                                overs.replace(\"&nbsp;\",'')\n",
    "                            jdict[i]['innings'][inning_order]['team_bowling'][team_bowling]['bowlers_all'][bowler_name]['maidens']=\\\n",
    "                                row.xpath('td[3]/text()')[0]\n",
    "                            jdict[i]['innings'][inning_order]['team_bowling'][team_bowling]['bowlers_all'][bowler_name]['runs']=\\\n",
    "                                row.xpath('td[4]/text()')[0]\n",
    "                            jdict[i]['innings'][inning_order]['team_bowling'][team_bowling]['bowlers_all'][bowler_name]['wickets']=\\\n",
    "                                row.xpath('td[5]/text()')[0]\n",
    "\n",
    "            print \"finished \" + i + '\\n'\n",
    "\n",
    "    #code below taken from http://stackoverflow.com/questions/2594810/removing-non-breaking-spaces-from-strings-using-python\n",
    "    #and http://stackoverflow.com/questions/12309269/how-do-i-write-json-data-to-a-file-in-python\n",
    "    \n",
    "    jstring = json.dumps(jdict, indent=4, sort_keys=True)\n",
    "    jstring2 = jstring.replace(\"\\u00a0\",\"\")\n",
    "\n",
    "    filenameg = formatx + '_data.json' \n",
    "    fnameg = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenameg)\n",
    "\n",
    "    with io.open(fnameg, 'w') as json_file:\n",
    "        json_file.write(unicode(jstring2))\n",
    "\n",
    "print \"Processing done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#am loading the different jsons that have been created into different dataframes just to do some preliminary \n",
    "#data checks and see if everything is kosher\n",
    "\n",
    "#import pandas as pd\n",
    "test_df = pd.read_json('test_data.json')\n",
    "odi_df = pd.read_json('odi_data.json')\n",
    "t20_df = pd.read_json('t20_data.json')\n",
    "\n",
    "#import json\n",
    "\n",
    "#with open('test_data.json', 'r') as f:\n",
    "#     test_df = json.load(f)\n",
    "\n",
    "#with open('odi_data.json', 'r') as f:\n",
    "#     odi_df = json.load(f)\n",
    "\n",
    "#with open('t20_data.json', 'r') as f:\n",
    "#     t20_df = json.load(f)\n",
    "\n",
    "nation_list = ['Australia','England','New Zealand','West Indies','India','Sri Lanka','Pakistan','Bangladesh',\\\n",
    "               'South Africa','Zimbabwe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No total in test_0046.html\n",
      "Check test_0046.html and team Sri Lanka and inning no. 2\n",
      "No total in test_0124.html\n",
      "Check test_0124.html and team India and inning no. 2\n",
      ".\n",
      ".\n",
      ".\n",
      "No total in t20_0538.html\n",
      "Check t20_0538.html and team Scotland and inning no. 1\n",
      "No total in t20_0538.html\n",
      "Check t20_0538.html and team Ireland and inning no. 2\n"
     ]
    }
   ],
   "source": [
    "#one of the checks i did, essentially here i'm asking the code to add the runs that each batsman has scored in an innings\n",
    "#, add it to the extras and see if it equals the total score that we'd extracted separately\n",
    "\n",
    "for formatx in [test_df, odi_df, t20_df]:\n",
    "    for page in formatx.keys():\n",
    "        #print page\n",
    "        try:\n",
    "            formatx[page]['innings'].keys()\n",
    "        except:\n",
    "            continue\n",
    "        else:\n",
    "            for inning in formatx[page]['innings'].keys():\n",
    "                for team in formatx[page]['innings'][inning]['team_batting'].keys():\n",
    "                    non_extras = 0\n",
    "                    try:\n",
    "                        extras = int(formatx[page]['innings'][inning]['team_batting'][team]['extras']['runs'])\n",
    "                    except:\n",
    "                        extras = 0\n",
    "                    try:\n",
    "                        total = int(formatx[page]['innings'][inning]['team_batting'][team]['total']['runs'])\n",
    "                    except:\n",
    "                        print \"No total in \" + page\n",
    "                    for position in formatx[page]['innings'][inning]['team_batting'][team]['batsman_all'].keys():\n",
    "                        for name in formatx[page]['innings'][inning]['team_batting'][team]['batsman_all'][position].keys():\n",
    "                            try:\n",
    "                                non_extras = non_extras + int(formatx[page]['innings'][inning]['team_batting'][team]\\\n",
    "                                                              ['batsman_all'][position][name]['batsman_runs'])\n",
    "                            except:\n",
    "                                non_extras = non_extras + 0\n",
    "                    if non_extras + extras != total:\n",
    "                        print \"Check \" + page + \" and team \" + team + \" and inning no. \" + inning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check test_0170.html\n",
      "check test_0292.html\n",
      "check test_0293.html\n",
      ".\n",
      ".\n",
      ".\n",
      "check t20_0566.html\n",
      "check t20_0612.html\n",
      "check t20_0614.html\n"
     ]
    }
   ],
   "source": [
    "#this is just another check of the data we've scraped\n",
    "#this code was done to find out matches where there results other than a victory or a draw, such as match tied, cancelled\n",
    "#suspended etc. see if there are any troublesome matches with weird scoreboards that might have produced\n",
    "#weird results even if no errors were put out during the processing\n",
    "\n",
    "\n",
    "#so it seems the text given out by doing [result_text] is unicode, doing 'if any in for that' as I've done below\n",
    "#is the easiest way to search for a substring in the text\n",
    "\n",
    "listx = ['won by','drawn']\n",
    "\n",
    "for formatx in [test_df, odi_df, t20_df]:\n",
    "    for page in formatx.keys():\n",
    "        if any(sub in formatx[page]['result_text'] for sub in listx):\n",
    "            continue\n",
    "        else:\n",
    "            print 'check ' + page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing done\n"
     ]
    }
   ],
   "source": [
    "#this block is to get the totals distributed by year\n",
    "#ie. how many totals between 0-200, how many between 201-400 etc.\n",
    "#note that this only considers those totals from ODIs and T20s where the team batted first and those totals from tests\n",
    "#made in a team's first innings\n",
    "\n",
    "#the reason why I'm only considering first innings totals is that first innings totals typically represent a team\n",
    "#trying to score as much as it can. The second innings totals are typically about setting a good enough or \n",
    "#safe enough target to set the other team. So if a team is set only 100 to win a test, this code doesn't penalise\n",
    "#them for only scoring that much. That was the target they were set, they could have scored more, but didn't need to.\n",
    "#Same is the case for T20 and ODI matches, only the totals of the teams batting first were recorded.\n",
    "\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "\n",
    "test_df.name = 'test_df'\n",
    "odi_df.name = 'odi_df'\n",
    "t20_df.name = 't20_df'\n",
    "\n",
    "for formatx in [test_df, odi_df, t20_df]:\n",
    "\n",
    "    jdict = defaultdict(lambda: defaultdict(lambda: defaultdict(int))) \n",
    "\n",
    "    for page in formatx.keys():\n",
    "        year = formatx[page]['year']\n",
    "        try:\n",
    "            formatx[page]['innings'].keys()\n",
    "        except:\n",
    "            continue\n",
    "        else:\n",
    "            for inning in formatx[page]['innings'].keys():\n",
    "                if formatx.name == test_df.name:\n",
    "                    if int(inning) == 1 or int(inning) == 2:\n",
    "                        for team in formatx[page]['innings'][inning]['team_batting'].keys():\n",
    "                            if team in nation_list:\n",
    "                                try:\n",
    "                                    int(formatx[page]['innings'][inning]['team_batting'][team]['total']['runs'])\n",
    "                                except:\n",
    "                                    continue\n",
    "                                else:\n",
    "                                    if int(formatx[page]['innings'][inning]['team_batting'][team]['total']['runs']) > 600:\n",
    "                                        jdict[team][year]['600 plus'] += 1\n",
    "                                    elif int(formatx[page]['innings'][inning]['team_batting'][team]['total']['runs']) > 400:\n",
    "                                        jdict[team][year]['401-600'] += 1\n",
    "                                    elif int(formatx[page]['innings'][inning]['team_batting'][team]['total']['runs']) > 200:\n",
    "                                        jdict[team][year]['201-400'] += 1\n",
    "                                    else:\n",
    "                                        jdict[team][year]['0-200'] += 1 #one\n",
    "                    else:\n",
    "                        continue\n",
    "                elif formatx.name == odi_df.name:\n",
    "                    if int(inning) == 1 or int(inning) == 2:\n",
    "                        for team in formatx[page]['innings'][inning]['team_batting'].keys():\n",
    "                            if team in nation_list:\n",
    "                                try:\n",
    "                                    int(formatx[page]['innings'][inning]['team_batting'][team]['total']['runs'])\n",
    "                                except:\n",
    "                                    continue\n",
    "                                else:\n",
    "                                    if int(formatx[page]['innings'][inning]['team_batting'][team]['total']['runs']) > 300:\n",
    "                                        jdict[team][year]['300 plus'] += 1\n",
    "                                    elif int(formatx[page]['innings'][inning]['team_batting'][team]['total']['runs']) > 200:\n",
    "                                        jdict[team][year]['201-300'] += 1\n",
    "                                    elif int(formatx[page]['innings'][inning]['team_batting'][team]['total']['runs']) > 100:\n",
    "                                        jdict[team][year]['101-200'] += 1\n",
    "                                    else:\n",
    "                                        jdict[team][year]['0-100'] += 1 #two\n",
    "                    else:\n",
    "                        continue\n",
    "                else:\n",
    "                    if int(inning) == 1:\n",
    "                        for team in formatx[page]['innings'][inning]['team_batting'].keys():\n",
    "                            if team in nation_list:\n",
    "                                try:\n",
    "                                    int(formatx[page]['innings'][inning]['team_batting'][team]['total']['runs'])\n",
    "                                except:\n",
    "                                    continue\n",
    "                                else:\n",
    "                                    if int(formatx[page]['innings'][inning]['team_batting'][team]['total']['runs']) > 200:\n",
    "                                        jdict[team][year]['200 plus'] += 1\n",
    "                                    elif int(formatx[page]['innings'][inning]['team_batting'][team]['total']['runs']) > 100:\n",
    "                                        jdict[team][year]['101-200'] += 1\n",
    "                                    else:\n",
    "                                        jdict[team][year]['0-100'] += 1 #three\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "    jstring = json.dumps(jdict, indent=4, sort_keys=True)\n",
    "    \n",
    "    filenameg = formatx.name + '_totals_by_year.json' \n",
    "    fnameg = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenameg)\n",
    "\n",
    "    with io.open(fnameg, 'w') as json_file:\n",
    "        json_file.write(unicode(jstring))\n",
    "\n",
    "print \"Processing done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this block is to get the individual scores distributed by year and team\n",
    "#individual scores made in all innings considered for this, no distinction between first innings and second innings\n",
    "#as in the code block above\n",
    "\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "\n",
    "test_df.name = 'test_df'\n",
    "odi_df.name = 'odi_df'\n",
    "t20_df.name = 't20_df'\n",
    "\n",
    "for formatx in [test_df, odi_df, t20_df]:\n",
    "\n",
    "    jdict = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "    \n",
    "    for page in formatx.keys():\n",
    "        print page\n",
    "        year = formatx[page]['year']\n",
    "        try:\n",
    "            formatx[page]['innings'].keys()\n",
    "        except:\n",
    "            continue\n",
    "        else:\n",
    "            for inning in formatx[page]['innings'].keys():\n",
    "                if formatx.name == test_df.name:\n",
    "                    for team in formatx[page]['innings'][inning]['team_batting'].keys():\n",
    "                        if team in nation_list:\n",
    "                            try:\n",
    "                                int(formatx[page]['innings'][inning]['team_batting'][team]['total']['runs'])\n",
    "                            except:\n",
    "                                continue\n",
    "                            else:\n",
    "                                for order in formatx[page]['innings'][inning]['team_batting'][team]['batsman_all'].keys():\n",
    "                                    for namex in formatx[page]['innings'][inning]['team_batting'][team]['batsman_all'][order].keys():\n",
    "                                        try:\n",
    "                                            int(formatx[page]['innings'][inning]['team_batting'][team]['batsman_all'][order]\\\n",
    "                                               [namex]['batsman_runs'])\n",
    "                                        except:\n",
    "                                            continue\n",
    "                                        else:\n",
    "                                            if int(formatx[page]['innings'][inning]['team_batting'][team]['batsman_all'][order]\\\n",
    "                                                   [namex]['batsman_runs']) > 299:\n",
    "                                                jdict[team][year]['300 & over'] += 1\n",
    "                                            elif int(formatx[page]['innings'][inning]['team_batting'][team]['batsman_all'][order]\\\n",
    "                                                   [namex]['batsman_runs']) > 199:\n",
    "                                                jdict[team][year]['200-299'] += 1\n",
    "                                            elif int(formatx[page]['innings'][inning]['team_batting'][team]['batsman_all'][order]\\\n",
    "                                                   [namex]['batsman_runs']) > 99:\n",
    "                                                jdict[team][year]['100-199'] += 1\n",
    "                                            else:\n",
    "                                                jdict[team][year]['0-99'] += 1\n",
    "                                                                        \n",
    "                elif formatx.name == odi_df.name:\n",
    "                    for team in formatx[page]['innings'][inning]['team_batting'].keys():\n",
    "                        if team in nation_list:\n",
    "                            try:\n",
    "                                int(formatx[page]['innings'][inning]['team_batting'][team]['total']['runs'])\n",
    "                            except:\n",
    "                                continue\n",
    "                            else:\n",
    "                                for order in formatx[page]['innings'][inning]['team_batting'][team]['batsman_all'].keys():\n",
    "                                    for namex in formatx[page]['innings'][inning]['team_batting'][team]['batsman_all'][order].keys():\n",
    "                                        try:\n",
    "                                            int(formatx[page]['innings'][inning]['team_batting'][team]['batsman_all'][order]\\\n",
    "                                               [namex]['batsman_runs'])\n",
    "                                        except:\n",
    "                                            continue\n",
    "                                        else:\n",
    "                                            if int(formatx[page]['innings'][inning]['team_batting'][team]['batsman_all'][order]\\\n",
    "                                                   [namex]['batsman_runs']) > 149:\n",
    "                                                jdict[team][year]['150 & over'] += 1\n",
    "                                            elif int(formatx[page]['innings'][inning]['team_batting'][team]['batsman_all'][order]\\\n",
    "                                                   [namex]['batsman_runs']) > 99:\n",
    "                                                jdict[team][year]['100-149'] += 1\n",
    "                                            elif int(formatx[page]['innings'][inning]['team_batting'][team]['batsman_all'][order]\\\n",
    "                                                   [namex]['batsman_runs']) > 49:\n",
    "                                                jdict[team][year]['50-99'] += 1\n",
    "                                            else:\n",
    "                                                jdict[team][year]['0-49'] += 1\n",
    "                else:\n",
    "                    for team in formatx[page]['innings'][inning]['team_batting'].keys():\n",
    "                        if team in nation_list:\n",
    "                            try:\n",
    "                                int(formatx[page]['innings'][inning]['team_batting'][team]['total']['runs'])\n",
    "                            except:\n",
    "                                continue\n",
    "                            else:\n",
    "                                for order in formatx[page]['innings'][inning]['team_batting'][team]['batsman_all'].keys():\n",
    "                                    for namex in formatx[page]['innings'][inning]['team_batting'][team]['batsman_all'][order].keys():\n",
    "                                        try:\n",
    "                                            int(formatx[page]['innings'][inning]['team_batting'][team]['batsman_all'][order]\\\n",
    "                                               [namex]['batsman_runs'])\n",
    "                                        except:\n",
    "                                            continue\n",
    "                                        else:\n",
    "                                            if int(formatx[page]['innings'][inning]['team_batting'][team]['batsman_all'][order]\\\n",
    "                                                   [namex]['batsman_runs']) > 149:\n",
    "                                                jdict[team][year]['150 & over'] += 1\n",
    "                                            elif int(formatx[page]['innings'][inning]['team_batting'][team]['batsman_all'][order]\\\n",
    "                                                   [namex]['batsman_runs']) > 99:\n",
    "                                                jdict[team][year]['100-149'] += 1\n",
    "                                            elif int(formatx[page]['innings'][inning]['team_batting'][team]['batsman_all'][order]\\\n",
    "                                                   [namex]['batsman_runs']) > 49:\n",
    "                                                jdict[team][year]['50-99'] += 1\n",
    "                                            else:\n",
    "                                                jdict[team][year]['0-49'] += 1\n",
    "                                        \n",
    "    jstring = json.dumps(jdict, indent=4, sort_keys=True)\n",
    "    \n",
    "    filenameg = formatx.name + '_indiv_scores_by_year.json' \n",
    "    fnameg = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenameg)\n",
    "\n",
    "    with io.open(fnameg, 'w') as json_file:\n",
    "        json_file.write(unicode(jstring))\n",
    "\n",
    "\n",
    "print \"Processing done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this block is to get the runs made and the number of overs taken to get the runs\n",
    "#made by each team in each year in each of the cricket formats. It also tallies number of wickets fallen\n",
    "\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "\n",
    "test_df.name = 'test_df'\n",
    "odi_df.name = 'odi_df'\n",
    "t20_df.name = 't20_df'\n",
    "\n",
    "for formatx in [test_df, odi_df, t20_df]:\n",
    "\n",
    "    jdict = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "    \n",
    "    for page in formatx.keys():\n",
    "        print page\n",
    "        year = formatx[page]['year']\n",
    "        try:\n",
    "            formatx[page]['innings'].keys()\n",
    "        except:\n",
    "            continue\n",
    "        else:\n",
    "            for inning in formatx[page]['innings'].keys():\n",
    "                for team in formatx[page]['innings'][inning]['team_batting'].keys():\n",
    "                    if team in nation_list:\n",
    "                        try:\n",
    "                            int(formatx[page]['innings'][inning]['team_batting'][team]['total']['runs'])\n",
    "                        except:\n",
    "                            continue\n",
    "                        else:\n",
    "                            jdict[team][year]['runs'] += float(formatx[page]['innings'][inning]['team_batting']\\\n",
    "                                                             [team]['total']['runs'])\n",
    "                            jdict[team][year]['overs'] += float(formatx[page]['innings'][inning]['team_batting']\\\n",
    "                                                              [team]['total']['overs'])\n",
    "                            jdict[team][year]['wickets'] += float(formatx[page]['innings'][inning]['team_batting']\\\n",
    "                                                              [team]['total']['wickets'])\n",
    "                                                                        \n",
    "    jstring = json.dumps(jdict, indent=4, sort_keys=True)\n",
    "    \n",
    "    filenameg = formatx.name + '_overs_runs_wickets_by_year.json' \n",
    "    fnameg = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenameg)\n",
    "\n",
    "    with io.open(fnameg, 'w') as json_file:\n",
    "        json_file.write(unicode(jstring))\n",
    "\n",
    "print \"Processing done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this block is to get the maidens and overs bowled by\n",
    "#each team in each year in each of the cricket formats\n",
    "\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "\n",
    "\n",
    "test_df.name = 'test_df'\n",
    "odi_df.name = 'odi_df'\n",
    "t20_df.name = 't20_df'\n",
    "\n",
    "for formatx in [test_df, odi_df, t20_df]:\n",
    "\n",
    "    jdict = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "    \n",
    "    for page in formatx.keys():\n",
    "        print page\n",
    "        year = formatx[page]['year']\n",
    "        try:\n",
    "            formatx[page]['innings'].keys()\n",
    "        except:\n",
    "            continue\n",
    "        else:\n",
    "            for inning in formatx[page]['innings'].keys():\n",
    "                try:\n",
    "                    formatx[page]['innings'][inning]['team_bowling'].keys()\n",
    "                except:\n",
    "                    continue\n",
    "                else:\n",
    "                    for team in formatx[page]['innings'][inning]['team_bowling'].keys():\n",
    "                        if team in nation_list:\n",
    "                            for bowler in formatx[page]['innings'][inning]['team_bowling'][team]['bowlers_all'].keys():\n",
    "                                jdict[team][year]['overs'] += float(formatx[page]['innings'][inning]['team_bowling']\\\n",
    "                                                                  [team]['bowlers_all'][bowler]['overs'])\n",
    "                                try:\n",
    "                                    float(formatx[page]['innings'][inning]['team_bowling'][team]['bowlers_all'][bowler]['maidens'])\n",
    "                                except:\n",
    "                                    continue\n",
    "                                else:\n",
    "                                    jdict[team][year]['maidens'] += float(formatx[page]['innings'][inning]['team_bowling']\\\n",
    "                                                                 [team]['bowlers_all'][bowler]['maidens'])\n",
    "                            \n",
    "                                                                        \n",
    "    jstring = json.dumps(jdict, indent=4, sort_keys=True)\n",
    "    \n",
    "    filenameg = formatx.name + '_maidens_overs_by_year.json' \n",
    "    fnameg = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenameg)\n",
    "\n",
    "    with io.open(fnameg, 'w') as json_file:\n",
    "        json_file.write(unicode(jstring))\n",
    "\n",
    "print \"Processing done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_0001.html\n",
      "test_0002.html\n",
      ".\n",
      ".\n",
      ".\n",
      "t20_0617.html\n",
      "Processing done\n"
     ]
    }
   ],
   "source": [
    "#this block is to get the number of boundaries, sixes and overs distributed by year and team\n",
    "#individual scores made in all innings considered for this\n",
    "\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "\n",
    "#trial_df = pd.read_json('trial3.json') #loading a smaller json just to check if the code's working\n",
    "\n",
    "#trial_df.name = 'trial_df'\n",
    "\n",
    "test_df.name = 'test_df'\n",
    "odi_df.name = 'odi_df'\n",
    "t20_df.name = 't20_df'\n",
    "\n",
    "#for formatx in [trial_df]:\n",
    "for formatx in [test_df, odi_df, t20_df]:\n",
    "\n",
    "    jdict = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "    \n",
    "    for page in formatx.keys():\n",
    "        print page\n",
    "        year = formatx[page]['year']\n",
    "        try:\n",
    "            formatx[page]['innings'].keys()\n",
    "        except:\n",
    "            continue\n",
    "        else:\n",
    "            for inning in formatx[page]['innings'].keys():\n",
    "                \n",
    "                for team in formatx[page]['innings'][inning]['team_batting'].keys():\n",
    "                    if team in nation_list:\n",
    "                        try:\n",
    "                            int(formatx[page]['innings'][inning]['team_batting'][team]['total']['runs'])\n",
    "                        except:\n",
    "                            continue\n",
    "                        else:\n",
    "                            overs = float(formatx[page]['innings'][inning]['team_batting'][team]['total']['overs'])\n",
    "                            jdict[team][year]['overs'] += overs\n",
    "\n",
    "                            for order in formatx[page]['innings'][inning]['team_batting'][team]['batsman_all'].keys():\n",
    "                                for namex in formatx[page]['innings'][inning]['team_batting'][team]['batsman_all'][order].keys():\n",
    "\n",
    "                                    try:\n",
    "                                        int(formatx[page]['innings'][inning]['team_batting'][team]['batsman_all'][order]\\\n",
    "                                           [namex]['batsman_4s'])\n",
    "                                    except:\n",
    "                                        pass\n",
    "                                    else:\n",
    "                                        jdict[team][year]['fours'] += int(formatx[page]['innings'][inning]['team_batting']\\\n",
    "                                                                          [team]['batsman_all'][order][namex]['batsman_4s'])\n",
    "\n",
    "                                    try:\n",
    "                                        int(formatx[page]['innings'][inning]['team_batting'][team]['batsman_all'][order]\\\n",
    "                                           [namex]['batsman_6s'])\n",
    "                                    except:\n",
    "                                        continue\n",
    "                                    else:\n",
    "                                        jdict[team][year]['sixes'] += int(formatx[page]['innings'][inning]['team_batting']\\\n",
    "                                                                          [team]['batsman_all'][order][namex]['batsman_6s'])\n",
    "\n",
    "                                            \n",
    "    jstring = json.dumps(jdict, indent=4, sort_keys=True)\n",
    "    \n",
    "    filenameg = formatx.name + '_fours_sixes_overs_by_year.json' \n",
    "    fnameg = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenameg)\n",
    "\n",
    "    with io.open(fnameg, 'w') as json_file:\n",
    "        json_file.write(unicode(jstring))\n",
    "\n",
    "print \"Processing done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_0001.html\n",
      "test_0002.html\n",
      ".\n",
      ".\n",
      ".\n",
      "t20_0616.html\n",
      "t20_0617.html\n",
      "Processing done\n"
     ]
    }
   ],
   "source": [
    "#this block is to find out how often a run out happens, how many overs on average between someone being run out\n",
    "\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "\n",
    "test_df.name = 'test_df'\n",
    "odi_df.name = 'odi_df'\n",
    "t20_df.name = 't20_df'\n",
    "\n",
    "for formatx in [test_df, odi_df, t20_df]:\n",
    "\n",
    "    jdict = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "    \n",
    "    for page in formatx.keys():\n",
    "        print page\n",
    "        year = formatx[page]['year']\n",
    "        try:\n",
    "            formatx[page]['innings'].keys()\n",
    "        except:\n",
    "            continue\n",
    "        else:\n",
    "            for inning in formatx[page]['innings'].keys():\n",
    "                for team in formatx[page]['innings'][inning]['team_batting'].keys():\n",
    "                    if team in nation_list:\n",
    "                        try:\n",
    "                            int(formatx[page]['innings'][inning]['team_batting'][team]['total']['runs'])\n",
    "                        except:\n",
    "                            continue\n",
    "                        else:\n",
    "                            jdict[team][year]['runs'] += float(formatx[page]['innings'][inning]['team_batting']\\\n",
    "                                                             [team]['total']['runs'])\n",
    "                            jdict[team][year]['overs'] += float(formatx[page]['innings'][inning]['team_batting']\\\n",
    "                                                              [team]['total']['overs'])\n",
    "                            jdict[team][year]['wickets'] += float(formatx[page]['innings'][inning]['team_batting']\\\n",
    "                                                              [team]['total']['wickets'])\n",
    "\n",
    "                            for order in formatx[page]['innings'][inning]['team_batting'][team]['batsman_all'].keys():\n",
    "                                for namex in formatx[page]['innings'][inning]['team_batting'][team]['batsman_all'][order].keys():\n",
    "                                    try:\n",
    "                                        formatx[page]['innings'][inning]['team_batting'][team]['batsman_all'][order]\\\n",
    "                                           [namex]['batsman_dismissal_text']\n",
    "                                    except:\n",
    "                                        continue\n",
    "                                    else:\n",
    "                                        if 'run out' in formatx[page]['innings'][inning]['team_batting'][team]['batsman_all'][order]\\\n",
    "                                           [namex]['batsman_dismissal_text']:\n",
    "                                                jdict[team][year]['run_outs'] += 1                                    \n",
    "                                    \n",
    "    jstring = json.dumps(jdict, indent=4, sort_keys=True)\n",
    "    \n",
    "    filenameg = formatx.name + '_run_outs_by_year.json' \n",
    "    fnameg = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenameg)\n",
    "\n",
    "    with io.open(fnameg, 'w') as json_file:\n",
    "        json_file.write(unicode(jstring))\n",
    "\n",
    "print \"Processing done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_0001.html\n",
      ".\n",
      ".\n",
      ".\n",
      "test_1124.html\n",
      "test_1125.html\n",
      "Processing done\n"
     ]
    }
   ],
   "source": [
    "#this block is to find out how often a draw happens in test matches\n",
    "\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "\n",
    "test_df.name = 'test_df'\n",
    "\n",
    "for formatx in [test_df]:\n",
    "\n",
    "    jdict = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    for page in formatx.keys():\n",
    "        print page\n",
    "        year = formatx[page]['year']\n",
    "        try:\n",
    "            formatx[page]['result_text']\n",
    "        except:\n",
    "            continue\n",
    "        else:\n",
    "            if 'won by' in formatx[page]['result_text']:\n",
    "                jdict[year]['win_loss'] += 1\n",
    "            elif 'drawn' in formatx[page]['result_text']:\n",
    "                jdict[year]['draw'] += 1\n",
    "            else:\n",
    "                jdict[year]['other_result'] += 1\n",
    "                                    \n",
    "    jstring = json.dumps(jdict, indent=4, sort_keys=True)\n",
    "    \n",
    "    filenameg = formatx.name + '_draws_by_year.json' \n",
    "    fnameg = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenameg)\n",
    "\n",
    "    with io.open(fnameg, 'w') as json_file:\n",
    "        json_file.write(unicode(jstring))\n",
    "\n",
    "print \"Processing done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing done\n"
     ]
    }
   ],
   "source": [
    "#this is some further processing of the data we'd extracted earlier. So we'd earlier gotten how many totals between\n",
    "#0-200, 200-400 etc. were scored by each team in a year. Here what we're doing is creating groups of five years each\n",
    "#say 1997-2001, 2002-2006 etc. and getting the figures for these five-year groups\n",
    "\n",
    "#am also creating a separate category that just totals up the figures for the 10 test-playing nations\n",
    "\n",
    "#also i'm taking only the last twenty years 1997 to 2016 for this\n",
    "\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "#test_df_totals = pd.read_json('test_df_totals_by_year.json')\n",
    "#didnt' use pandas to load the json here, pandas was creating keys where there weren't any and was giving me lots of errors\n",
    "#used the basic json python library instead\n",
    "\n",
    "with open('test_df_totals_by_year.json', 'r') as f:\n",
    "     test_df_totals = json.load(f)\n",
    "\n",
    "jdict = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "\n",
    "\n",
    "first_band = [\"1997\",\"1998\",\"1999\",\"2000\",\"2001\"]\n",
    "second_band = [\"2002\",\"2003\",\"2004\",\"2005\",\"2006\"]\n",
    "third_band = [\"2007\",\"2008\",\"2009\",\"2010\",\"2011\"]\n",
    "fourth_band = [\"2012\",\"2013\",\"2014\",\"2015\",\"2016\"]\n",
    "\n",
    "\n",
    "for nation in test_df_totals.keys():\n",
    "    if nation in ['India','Pakistan','Sri Lanka']:\n",
    "        for year in test_df_totals[nation].keys():\n",
    "            #year = year.astype(np.int64)\n",
    "            if year in first_band:\n",
    "                for rangex in test_df_totals[nation][year].keys():\n",
    "                    jdict[nation]['1997-2001'][rangex] += test_df_totals[nation][year][rangex]\n",
    "            elif year in second_band:\n",
    "                for rangex in test_df_totals[nation][year].keys():\n",
    "                    jdict[nation]['2002-2006'][rangex] += test_df_totals[nation][year][rangex]\n",
    "            elif year in third_band:\n",
    "                #year = str(year)\n",
    "                for rangex in test_df_totals[nation][year].keys():\n",
    "                    jdict[nation]['2007-2011'][rangex] += test_df_totals[nation][year][rangex]\n",
    "            elif year in fourth_band:\n",
    "                for rangex in test_df_totals[nation][year].keys():\n",
    "                    jdict[nation]['2012-2016'][rangex] += test_df_totals[nation][year][rangex]\n",
    "\n",
    "for nation in test_df_totals.keys():\n",
    "    for year in test_df_totals[nation].keys():\n",
    "        #year = year.astype(np.int64)\n",
    "        if year in first_band:\n",
    "            for rangex in test_df_totals[nation][year].keys():\n",
    "                jdict['Test_nations']['1997-2001'][rangex] += test_df_totals[nation][year][rangex]\n",
    "        elif year in second_band:\n",
    "            for rangex in test_df_totals[nation][year].keys():\n",
    "                jdict['Test_nations']['2002-2006'][rangex] += test_df_totals[nation][year][rangex]\n",
    "        elif year in third_band:\n",
    "            for rangex in test_df_totals[nation][year].keys():\n",
    "                jdict['Test_nations']['2007-2011'][rangex] += test_df_totals[nation][year][rangex]\n",
    "        elif year in fourth_band:\n",
    "            for rangex in test_df_totals[nation][year].keys():\n",
    "                jdict['Test_nations']['2012-2016'][rangex] += test_df_totals[nation][year][rangex]\n",
    "\n",
    "\n",
    "jstring = json.dumps(jdict, indent=4, sort_keys=True)\n",
    "\n",
    "filenameg = 'final_test_totals.json' \n",
    "fnameg = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenameg)\n",
    "\n",
    "with io.open(fnameg, 'w') as json_file:\n",
    "    json_file.write(unicode(jstring))\n",
    "\n",
    "print \"Processing done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing done\n"
     ]
    }
   ],
   "source": [
    "#same purpose as the code block above, except this is for ODIs\n",
    "\n",
    "with open('odi_df_totals_by_year.json', 'r') as f:\n",
    "     odi_df_totals = json.load(f)\n",
    "\n",
    "jdict = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "\n",
    "\n",
    "first_band = [\"1997\",\"1998\",\"1999\",\"2000\",\"2001\"]\n",
    "second_band = [\"2002\",\"2003\",\"2004\",\"2005\",\"2006\"]\n",
    "third_band = [\"2007\",\"2008\",\"2009\",\"2010\",\"2011\"]\n",
    "fourth_band = [\"2012\",\"2013\",\"2014\",\"2015\",\"2016\"]\n",
    "\n",
    "\n",
    "for nation in odi_df_totals.keys():\n",
    "    if nation in ['India','Pakistan','Sri Lanka']:\n",
    "        for year in odi_df_totals[nation].keys():\n",
    "            #year = year.astype(np.int64)\n",
    "            if year in first_band:\n",
    "                for rangex in odi_df_totals[nation][year].keys():\n",
    "                    jdict[nation]['1997-2001'][rangex] += odi_df_totals[nation][year][rangex]\n",
    "            elif year in second_band:\n",
    "                for rangex in odi_df_totals[nation][year].keys():\n",
    "                    jdict[nation]['2002-2006'][rangex] += odi_df_totals[nation][year][rangex]\n",
    "            elif year in third_band:\n",
    "                #year = str(year)\n",
    "                for rangex in odi_df_totals[nation][year].keys():\n",
    "                    jdict[nation]['2007-2011'][rangex] += odi_df_totals[nation][year][rangex]\n",
    "            elif year in fourth_band:\n",
    "                for rangex in odi_df_totals[nation][year].keys():\n",
    "                    jdict[nation]['2012-2016'][rangex] += odi_df_totals[nation][year][rangex]\n",
    "\n",
    "for nation in odi_df_totals.keys():\n",
    "    for year in odi_df_totals[nation].keys():\n",
    "        if year in first_band:\n",
    "            for rangex in odi_df_totals[nation][year].keys():\n",
    "                jdict['Test_nations']['1997-2001'][rangex] += odi_df_totals[nation][year][rangex]\n",
    "        elif year in second_band:\n",
    "            for rangex in odi_df_totals[nation][year].keys():\n",
    "                jdict['Test_nations']['2002-2006'][rangex] += odi_df_totals[nation][year][rangex]\n",
    "        elif year in third_band:\n",
    "            for rangex in odi_df_totals[nation][year].keys():\n",
    "                jdict['Test_nations']['2007-2011'][rangex] += odi_df_totals[nation][year][rangex]\n",
    "        elif year in fourth_band:\n",
    "            for rangex in odi_df_totals[nation][year].keys():\n",
    "                jdict['Test_nations']['2012-2016'][rangex] += odi_df_totals[nation][year][rangex]\n",
    "\n",
    "\n",
    "jstring = json.dumps(jdict, indent=4, sort_keys=True)\n",
    "\n",
    "filenameg = 'final_odi_totals.json' \n",
    "fnameg = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenameg)\n",
    "\n",
    "with io.open(fnameg, 'w') as json_file:\n",
    "    json_file.write(unicode(jstring))\n",
    "\n",
    "print \"Processing done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing done\n"
     ]
    }
   ],
   "source": [
    "#this takes the number of centuries, double and triple centuries scored by individual batsmen in each year\n",
    "#totals them up for each five-year group 1997-2001 , 2002-2006 etc.\n",
    "#it also does it for all the Test nations\n",
    "\n",
    "with open('test_df_indiv_scores_by_year.json', 'r') as f:\n",
    "     test_df_indiv_scores = json.load(f)\n",
    "\n",
    "jdict = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "\n",
    "\n",
    "first_band = [\"1997\",\"1998\",\"1999\",\"2000\",\"2001\"]\n",
    "second_band = [\"2002\",\"2003\",\"2004\",\"2005\",\"2006\"]\n",
    "third_band = [\"2007\",\"2008\",\"2009\",\"2010\",\"2011\"]\n",
    "fourth_band = [\"2012\",\"2013\",\"2014\",\"2015\",\"2016\"]\n",
    "\n",
    "\n",
    "for nation in test_df_indiv_scores.keys():\n",
    "    if nation in ['India','Pakistan','Sri Lanka']:\n",
    "        for year in test_df_indiv_scores[nation].keys():\n",
    "            if year in first_band:\n",
    "                for rangex in test_df_indiv_scores[nation][year].keys():\n",
    "                    if rangex != '0-99':\n",
    "                        jdict[nation]['1997-2001'][rangex] += test_df_indiv_scores[nation][year][rangex]\n",
    "            elif year in second_band:\n",
    "                for rangex in test_df_indiv_scores[nation][year].keys():\n",
    "                    if rangex != '0-99':\n",
    "                        jdict[nation]['2002-2006'][rangex] += test_df_indiv_scores[nation][year][rangex]\n",
    "            elif year in third_band:\n",
    "                for rangex in test_df_indiv_scores[nation][year].keys():\n",
    "                    if rangex != '0-99':\n",
    "                        jdict[nation]['2007-2011'][rangex] += test_df_indiv_scores[nation][year][rangex]\n",
    "            elif year in fourth_band:\n",
    "                for rangex in test_df_indiv_scores[nation][year].keys():\n",
    "                    if rangex != '0-99':\n",
    "                        jdict[nation]['2012-2016'][rangex] += test_df_indiv_scores[nation][year][rangex]\n",
    "\n",
    "for nation in test_df_indiv_scores.keys():\n",
    "    for year in test_df_indiv_scores[nation].keys():\n",
    "        if year in first_band:\n",
    "            for rangex in test_df_indiv_scores[nation][year].keys():\n",
    "                if rangex != '0-99':\n",
    "                    jdict['Test_nations']['1997-2001'][rangex] += test_df_indiv_scores[nation][year][rangex]\n",
    "        elif year in second_band:\n",
    "            for rangex in test_df_indiv_scores[nation][year].keys():\n",
    "                if rangex != '0-99':\n",
    "                    jdict['Test_nations']['2002-2006'][rangex] += test_df_indiv_scores[nation][year][rangex]\n",
    "        elif year in third_band:\n",
    "            for rangex in test_df_indiv_scores[nation][year].keys():\n",
    "                if rangex != '0-99':\n",
    "                    jdict['Test_nations']['2007-2011'][rangex] += test_df_indiv_scores[nation][year][rangex]\n",
    "        elif year in fourth_band:\n",
    "            for rangex in test_df_indiv_scores[nation][year].keys():\n",
    "                if rangex != '0-99':\n",
    "                    jdict['Test_nations']['2012-2016'][rangex] += test_df_indiv_scores[nation][year][rangex]\n",
    "\n",
    "\n",
    "jstring = json.dumps(jdict, indent=4, sort_keys=True)\n",
    "\n",
    "filenameg = 'final_test_indiv_scores.json' \n",
    "fnameg = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenameg)\n",
    "\n",
    "with io.open(fnameg, 'w') as json_file:\n",
    "    json_file.write(unicode(jstring))\n",
    "\n",
    "print \"Processing done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing done\n"
     ]
    }
   ],
   "source": [
    "#same motivation as the code block above, just that this is done for ODIs\n",
    "\n",
    "with open('odi_df_indiv_scores_by_year.json', 'r') as f:\n",
    "     odi_df_indiv_scores = json.load(f)\n",
    "\n",
    "jdict = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "\n",
    "first_band = [\"1997\",\"1998\",\"1999\",\"2000\",\"2001\"]\n",
    "second_band = [\"2002\",\"2003\",\"2004\",\"2005\",\"2006\"]\n",
    "third_band = [\"2007\",\"2008\",\"2009\",\"2010\",\"2011\"]\n",
    "fourth_band = [\"2012\",\"2013\",\"2014\",\"2015\",\"2016\"]\n",
    "\n",
    "for nation in odi_df_indiv_scores.keys():\n",
    "    if nation in ['India','Pakistan','Sri Lanka']:\n",
    "        for year in odi_df_indiv_scores[nation].keys():\n",
    "            if year in first_band:\n",
    "                for rangex in odi_df_indiv_scores[nation][year].keys():\n",
    "                    if rangex != '0-49':\n",
    "                        jdict[nation]['1997-2001'][rangex] += odi_df_indiv_scores[nation][year][rangex]\n",
    "            elif year in second_band:\n",
    "                for rangex in odi_df_indiv_scores[nation][year].keys():\n",
    "                    if rangex != '0-49':\n",
    "                        jdict[nation]['2002-2006'][rangex] += odi_df_indiv_scores[nation][year][rangex]\n",
    "            elif year in third_band:\n",
    "                for rangex in odi_df_indiv_scores[nation][year].keys():\n",
    "                    if rangex != '0-49':\n",
    "                        jdict[nation]['2007-2011'][rangex] += odi_df_indiv_scores[nation][year][rangex]\n",
    "            elif year in fourth_band:\n",
    "                for rangex in odi_df_indiv_scores[nation][year].keys():\n",
    "                    if rangex != '0-49':\n",
    "                        jdict[nation]['2012-2016'][rangex] += odi_df_indiv_scores[nation][year][rangex]\n",
    "\n",
    "for nation in odi_df_indiv_scores.keys():\n",
    "    for year in odi_df_indiv_scores[nation].keys():\n",
    "        if year in first_band:\n",
    "            for rangex in odi_df_indiv_scores[nation][year].keys():\n",
    "                if rangex != '0-49':\n",
    "                    jdict['test_nations']['1997-2001'][rangex] += odi_df_indiv_scores[nation][year][rangex]\n",
    "        elif year in second_band:\n",
    "            for rangex in odi_df_indiv_scores[nation][year].keys():\n",
    "                if rangex != '0-49':\n",
    "                    jdict['test_nations']['2002-2006'][rangex] += odi_df_indiv_scores[nation][year][rangex]\n",
    "        elif year in third_band:\n",
    "            for rangex in odi_df_indiv_scores[nation][year].keys():\n",
    "                if rangex != '0-49':\n",
    "                    jdict['test_nations']['2007-2011'][rangex] += odi_df_indiv_scores[nation][year][rangex]\n",
    "        elif year in fourth_band:\n",
    "            for rangex in odi_df_indiv_scores[nation][year].keys():\n",
    "                if rangex != '0-49':\n",
    "                    jdict['test_nations']['2012-2016'][rangex] += odi_df_indiv_scores[nation][year][rangex]\n",
    "\n",
    "\n",
    "jstring = json.dumps(jdict, indent=4, sort_keys=True)\n",
    "\n",
    "filenameg = 'final_odi_indiv_scores.json' \n",
    "fnameg = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenameg)\n",
    "\n",
    "with io.open(fnameg, 'w') as json_file:\n",
    "    json_file.write(unicode(jstring))\n",
    "\n",
    "print \"Processing done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing done\n"
     ]
    }
   ],
   "source": [
    "#while I was doing this code block, I thought of having four separate categories, India, Pakistan, Sri Lanka and\n",
    "# 'all test nations together' . Decided later against having more than one category as it would have just cluttered up\n",
    "#my graphics.\n",
    "\n",
    "#but anyway what this code is create a json file with the number overs bowled in a year, the no. of wickets fallen and \n",
    "#number of runs scored. So this would tell you the rate at which runs have been scored in a year or the frequency\n",
    "#with which wickets have fallen\n",
    "\n",
    "\n",
    "with open('test_df_overs_runs_wickets_by_year.json', 'r') as f:\n",
    "     test_df_run_wicket_rate = json.load(f)\n",
    "\n",
    "jdict = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "\n",
    "\n",
    "for nation in test_df_run_wicket_rate.keys():\n",
    "    if nation in ['India','Pakistan','Sri Lanka']:\n",
    "        for year in test_df_run_wicket_rate[nation].keys():\n",
    "            if year not in ['1990','2017']:\n",
    "                jdict[nation][year]['run_rate'] = (test_df_run_wicket_rate[nation][year]['runs']/test_df_run_wicket_rate[nation]\\\n",
    "                    [year]['overs'])*90\n",
    "                jdict[nation][year]['wicket_rate'] = test_df_run_wicket_rate[nation][year]['overs']/\\\n",
    "                                                    test_df_run_wicket_rate[nation][year]['wickets']\n",
    "\n",
    "\n",
    "for nation in test_df_run_wicket_rate.keys():\n",
    "    for year in test_df_run_wicket_rate[nation].keys():\n",
    "        if year not in ['1990','2017']:\n",
    "            jdict['test_nations'][year]['runs'] += test_df_run_wicket_rate[nation][year]['runs']\n",
    "            jdict['test_nations'][year]['wickets'] += test_df_run_wicket_rate[nation][year]['wickets']\n",
    "            jdict['test_nations'][year]['overs'] += test_df_run_wicket_rate[nation][year]['overs']\n",
    "            jdict['test_nations'][year]['wicket_rate'] = jdict['test_nations'][year]['overs']/jdict['test_nations'][year]['wickets']\n",
    "            jdict['test_nations'][year]['run_rate'] =(jdict['test_nations'][year]['runs']/jdict['test_nations'][year]['overs'])*90\n",
    "\n",
    "jstring = json.dumps(jdict, indent=4, sort_keys=True)\n",
    "\n",
    "filenameg = 'final_test_run_wicket_rate.json' \n",
    "fnameg = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenameg)\n",
    "\n",
    "with io.open(fnameg, 'w') as json_file:\n",
    "    json_file.write(unicode(jstring))\n",
    "\n",
    "print \"Processing done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing done\n"
     ]
    }
   ],
   "source": [
    "#same motivation as the code block above, just that it's for ODIs\n",
    "\n",
    "with open('odi_df_overs_runs_wickets_by_year.json', 'r') as f:\n",
    "     odi_df_run_wicket_rate = json.load(f)\n",
    "\n",
    "jdict = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "\n",
    "for nation in odi_df_run_wicket_rate.keys():\n",
    "    if nation in ['India','Pakistan','Sri Lanka']:\n",
    "        for year in odi_df_run_wicket_rate[nation].keys():\n",
    "            if year not in ['1990','2017']:\n",
    "                jdict[nation][year]['run_rate'] = (odi_df_run_wicket_rate[nation][year]['runs']/odi_df_run_wicket_rate[nation]\\\n",
    "                    [year]['overs'])*50\n",
    "                jdict[nation][year]['wicket_rate'] = odi_df_run_wicket_rate[nation][year]['overs']/odi_df_run_wicket_rate[nation]\\\n",
    "                                                      [year]['wickets']\n",
    "                #this looks at how often a wicket falls, ever xx overs, you don't need to *90 for this\n",
    "\n",
    "\n",
    "for nation in odi_df_run_wicket_rate.keys():\n",
    "    for year in odi_df_run_wicket_rate[nation].keys():\n",
    "        if year not in ['1990','2017']:\n",
    "            jdict['test_nations'][year]['runs'] += odi_df_run_wicket_rate[nation][year]['runs']\n",
    "            jdict['test_nations'][year]['wickets'] += odi_df_run_wicket_rate[nation][year]['wickets']\n",
    "            jdict['test_nations'][year]['overs'] += odi_df_run_wicket_rate[nation][year]['overs']\n",
    "            jdict['test_nations'][year]['wicket_rate'] = jdict['test_nations'][year]['overs']/jdict['test_nations'][year]['wickets']\n",
    "            jdict['test_nations'][year]['run_rate'] =(jdict['test_nations'][year]['runs']/jdict['test_nations'][year]['overs'])*50\n",
    "\n",
    "jstring = json.dumps(jdict, indent=4, sort_keys=True)\n",
    "\n",
    "filenameg = 'final_odi_run_wicket_rate.json' \n",
    "fnameg = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenameg)\n",
    "\n",
    "with io.open(fnameg, 'w') as json_file:\n",
    "    json_file.write(unicode(jstring))\n",
    "\n",
    "print \"Processing done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing done\n"
     ]
    }
   ],
   "source": [
    "#this looks at what percent of overs in a year are maiden overs\n",
    "\n",
    "with open('test_df_maidens_overs_by_year.json', 'r') as f:\n",
    "     test_df_maidens = json.load(f)\n",
    "\n",
    "jdict = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "\n",
    "for nation in test_df_maidens.keys():\n",
    "    if nation in ['India','Pakistan','Sri Lanka']:\n",
    "        for year in test_df_maidens[nation].keys():\n",
    "            if year not in ['1990','2017']:\n",
    "                jdict[nation][year]['maiden_rate'] =\\\n",
    "                    (test_df_maidens[nation][year]['maidens']/test_df_maidens[nation][year]['overs'])*100\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "for nation in test_df_maidens.keys():\n",
    "    for year in test_df_maidens[nation].keys():\n",
    "        if year not in ['1990','2017']:\n",
    "            jdict['test_nations'][year]['maidens'] += test_df_maidens[nation][year]['maidens']\n",
    "            jdict['test_nations'][year]['overs'] += test_df_maidens[nation][year]['overs']\n",
    "            jdict['test_nations'][year]['maiden_rate'] =\\\n",
    "                (jdict['test_nations'][year]['maidens']/jdict['test_nations'][year]['overs'])*100\n",
    "\n",
    "jstring = json.dumps(jdict, indent=4, sort_keys=True)\n",
    "\n",
    "filenameg = 'final_test_maidens_rate.json' \n",
    "fnameg = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenameg)\n",
    "\n",
    "with io.open(fnameg, 'w') as json_file:\n",
    "    json_file.write(unicode(jstring))\n",
    "\n",
    "print \"Processing done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing done\n"
     ]
    }
   ],
   "source": [
    "#same purpose as the code block above, just that this is just for ODIs\n",
    "\n",
    "with open('odi_df_maidens_overs_by_year.json', 'r') as f:\n",
    "     odi_df_maidens = json.load(f)\n",
    "\n",
    "jdict = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "\n",
    "for nation in odi_df_maidens.keys():\n",
    "    if nation in ['India','Pakistan','Sri Lanka']:\n",
    "        for year in odi_df_maidens[nation].keys():\n",
    "            if year not in ['1990','2017']:\n",
    "                jdict[nation][year]['maiden_rate'] =\\\n",
    "                    (odi_df_maidens[nation][year]['maidens']/odi_df_maidens[nation][year]['overs'])*100\n",
    "\n",
    "\n",
    "\n",
    "for nation in odi_df_maidens.keys():\n",
    "    for year in odi_df_maidens[nation].keys():\n",
    "        if year not in ['1990','2017']:\n",
    "            jdict['test_nations'][year]['maidens'] += odi_df_maidens[nation][year]['maidens']\n",
    "            jdict['test_nations'][year]['overs'] += odi_df_maidens[nation][year]['overs']\n",
    "            jdict['test_nations'][year]['maiden_rate'] =\\\n",
    "                (jdict['test_nations'][year]['maidens']/jdict['test_nations'][year]['overs'])*100\n",
    "\n",
    "jstring = json.dumps(jdict, indent=4, sort_keys=True)\n",
    "\n",
    "filenameg = 'final_odi_maidens_rate.json' \n",
    "fnameg = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenameg)\n",
    "\n",
    "with io.open(fnameg, 'w') as json_file:\n",
    "    json_file.write(unicode(jstring))\n",
    "\n",
    "print \"Processing done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing done\n"
     ]
    }
   ],
   "source": [
    "#this tell you how many fours and sixes have been scored in test matches in a year\n",
    "\n",
    "with open('test_df_fours_sixes_overs_by_year.json', 'r') as f:\n",
    "     test_df_fours_sixes = json.load(f)\n",
    "\n",
    "jdict = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "\n",
    "for nation in test_df_fours_sixes.keys():\n",
    "    if nation in ['India','Pakistan','Sri Lanka']:\n",
    "        for year in test_df_fours_sixes[nation].keys():\n",
    "            if year not in ['1990','2017']:\n",
    "                jdict[nation][year]['fours_rate'] = \\\n",
    "                    test_df_fours_sixes[nation][year]['overs']/test_df_fours_sixes[nation][year]['fours']\n",
    "                if 'sixes' in test_df_fours_sixes[nation][year].keys():\n",
    "                    jdict[nation][year]['sixes_rate'] = \\\n",
    "                        test_df_fours_sixes[nation][year]['overs']/test_df_fours_sixes[nation][year]['sixes']\n",
    "\n",
    "                #this looks at how often we see a four or a six, every xx overs\n",
    "\n",
    "for nation in test_df_fours_sixes.keys():\n",
    "    for year in test_df_fours_sixes[nation].keys():\n",
    "        if year not in ['1990','2017']:\n",
    "            jdict['test_nations'][year]['fours'] += test_df_fours_sixes[nation][year]['fours']\n",
    "            jdict['test_nations'][year]['overs'] += test_df_fours_sixes[nation][year]['overs']\n",
    "            jdict['test_nations'][year]['fours_rate'] = jdict['test_nations'][year]['overs']/jdict['test_nations'][year]['fours']\n",
    "            if 'sixes' in test_df_fours_sixes[nation][year].keys():\n",
    "                jdict['test_nations'][year]['sixes'] += test_df_fours_sixes[nation][year]['sixes']\n",
    "                jdict['test_nations'][year]['sixes_rate'] = jdict['test_nations'][year]['overs']/jdict['test_nations'][year]['sixes']\n",
    "\n",
    "jstring = json.dumps(jdict, indent=4, sort_keys=True)\n",
    "\n",
    "filenameg = 'final_test_four_six_rate.json' \n",
    "fnameg = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenameg)\n",
    "\n",
    "with io.open(fnameg, 'w') as json_file:\n",
    "    json_file.write(unicode(jstring))\n",
    "\n",
    "print \"Processing done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing done\n"
     ]
    }
   ],
   "source": [
    "#same purpose as the code block above, just that this is for ODIs\n",
    "\n",
    "with open('odi_df_fours_sixes_overs_by_year.json', 'r') as f:\n",
    "     odi_df_fours_sixes = json.load(f)\n",
    "\n",
    "jdict = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "\n",
    "for nation in odi_df_fours_sixes.keys():\n",
    "    if nation in ['India','Pakistan','Sri Lanka']:\n",
    "        for year in odi_df_fours_sixes[nation].keys():\n",
    "            if year not in ['1990','2017']:\n",
    "                if 'fours' in odi_df_fours_sixes[nation][year].keys():\n",
    "                    jdict[nation][year]['fours_rate'] = \\\n",
    "                        odi_df_fours_sixes[nation][year]['overs']/odi_df_fours_sixes[nation][year]['fours']\n",
    "                if 'sixes' in odi_df_fours_sixes[nation][year].keys():\n",
    "                    jdict[nation][year]['sixes_rate'] = \\\n",
    "                        odi_df_fours_sixes[nation][year]['overs']/odi_df_fours_sixes[nation][year]['sixes']\n",
    "\n",
    "                #this looks at how often we see a four or a six, every xx overs\n",
    "\n",
    "for nation in odi_df_fours_sixes.keys():\n",
    "    for year in odi_df_fours_sixes[nation].keys():\n",
    "        if year not in ['1990','2017']:\n",
    "            jdict['test_nations'][year]['fours'] += odi_df_fours_sixes[nation][year]['fours']\n",
    "            jdict['test_nations'][year]['overs'] += odi_df_fours_sixes[nation][year]['overs']\n",
    "            jdict['test_nations'][year]['fours_rate'] = jdict['test_nations'][year]['overs']/jdict['test_nations'][year]['fours']\n",
    "            if 'sixes' in odi_df_fours_sixes[nation][year].keys():\n",
    "                jdict['test_nations'][year]['sixes'] += odi_df_fours_sixes[nation][year]['sixes']\n",
    "                jdict['test_nations'][year]['sixes_rate'] = jdict['test_nations'][year]['overs']/jdict['test_nations'][year]['sixes']\n",
    "\n",
    "jstring = json.dumps(jdict, indent=4, sort_keys=True)\n",
    "\n",
    "filenameg = 'final_odi_four_six_rate.json' \n",
    "fnameg = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenameg)\n",
    "\n",
    "with io.open(fnameg, 'w') as json_file:\n",
    "    json_file.write(unicode(jstring))\n",
    "\n",
    "print \"Processing done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing done\n"
     ]
    }
   ],
   "source": [
    "#this looks at what percentage of test match dismissals are run outs in a particular year\n",
    "\n",
    "with open('test_df_run_outs_by_year.json', 'r') as f:\n",
    "     test_df_run_outs = json.load(f)\n",
    "\n",
    "jdict = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "\n",
    "for nation in test_df_run_outs.keys():\n",
    "    if nation in ['India','Pakistan','Sri Lanka']:\n",
    "        for year in test_df_run_outs[nation].keys():\n",
    "            if year not in ['1990','2017']:\n",
    "                if 'run_outs' in test_df_run_outs[nation][year].keys():\n",
    "                    jdict[nation][year]['run_out_rate'] = \\\n",
    "                        (test_df_run_outs[nation][year]['run_outs']/test_df_run_outs[nation][year]['wickets'])*100\n",
    "                \n",
    "\n",
    "\n",
    "for nation in test_df_run_outs.keys():\n",
    "    #print nation\n",
    "    for year in test_df_run_outs[nation].keys():\n",
    "        if year not in ['1990','2017']:\n",
    "            jdict['test_nations'][year]['wickets'] += test_df_run_outs[nation][year]['wickets']\n",
    "            #print year\n",
    "            try:\n",
    "                test_df_run_outs[nation][year]['run_outs']\n",
    "            except:\n",
    "                continue\n",
    "            else:\n",
    "                jdict['test_nations'][year]['run_outs'] += test_df_run_outs[nation][year]['run_outs']\n",
    "                jdict['test_nations'][year]['run_out_rate'] = \\\n",
    "                    (jdict['test_nations'][year]['run_outs']/jdict['test_nations'][year]['wickets'])*100\n",
    "\n",
    "jstring = json.dumps(jdict, indent=4, sort_keys=True)\n",
    "\n",
    "filenameg = 'final_test_run_out_rate.json' \n",
    "fnameg = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenameg)\n",
    "\n",
    "with io.open(fnameg, 'w') as json_file:\n",
    "    json_file.write(unicode(jstring))\n",
    "\n",
    "print \"Processing done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing done\n"
     ]
    }
   ],
   "source": [
    "#same purpose as the code block above, just that this is for ODIs\n",
    "\n",
    "with open('odi_df_run_outs_by_year.json', 'r') as f:\n",
    "     odi_df_run_outs = json.load(f)\n",
    "\n",
    "jdict = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "\n",
    "for nation in odi_df_run_outs.keys():\n",
    "    if nation in ['India','Pakistan','Sri Lanka']:\n",
    "        for year in odi_df_run_outs[nation].keys():\n",
    "            if year not in ['1990','2017']:\n",
    "                if 'run_outs' in odi_df_run_outs[nation][year].keys():\n",
    "                    jdict[nation][year]['run_out_rate'] = \\\n",
    "                        (odi_df_run_outs[nation][year]['run_outs']/odi_df_run_outs[nation][year]['wickets'])*100\n",
    "                \n",
    "\n",
    "\n",
    "for nation in odi_df_run_outs.keys():\n",
    "    for year in odi_df_run_outs[nation].keys():\n",
    "        if year not in ['1990','2017']:\n",
    "            jdict['test_nations'][year]['wickets'] += odi_df_run_outs[nation][year]['wickets']\n",
    "            if 'run_outs' in odi_df_run_outs[nation][year].keys():\n",
    "                jdict['test_nations'][year]['run_outs'] += odi_df_run_outs[nation][year]['run_outs']\n",
    "                jdict['test_nations'][year]['run_out_rate'] = \\\n",
    "                    (jdict['test_nations'][year]['run_outs']/jdict['test_nations'][year]['wickets'])*100\n",
    "\n",
    "jstring = json.dumps(jdict, indent=4, sort_keys=True)\n",
    "\n",
    "filenameg = 'final_odi_run_out_rate.json' \n",
    "fnameg = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenameg)\n",
    "\n",
    "with io.open(fnameg, 'w') as json_file:\n",
    "    json_file.write(unicode(jstring))\n",
    "\n",
    "print \"Processing done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing done\n"
     ]
    }
   ],
   "source": [
    "#this looks at what percentage of test matches in each year are drawn\n",
    "\n",
    "with open('test_df_draws_by_year.json', 'r') as f:\n",
    "     test_df_draws = json.load(f)\n",
    "\n",
    "jdict = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "for year in test_df_draws.keys():\n",
    "    if year not in ['1990','2017']:\n",
    "        for result in test_df_draws[year].keys():\n",
    "            jdict[year]['total_tests'] += test_df_draws[year][result]\n",
    "            \n",
    "            jdict[year]['draw_percent'] = (test_df_draws[year]['draw']/jdict[year]['total_tests'])*100\n",
    "            \n",
    "            #note that the draw percent is actually calculated as many times as there are 'result' in [year].keys\n",
    "            #it's ok this time, as the result was still calculated correctly, but you might want to keep that out of the \n",
    "            #loop next time\n",
    "        \n",
    "jstring = json.dumps(jdict, indent=4, sort_keys=True)\n",
    "\n",
    "filenameg = 'final_draw_percent.json' \n",
    "fnameg = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenameg)\n",
    "\n",
    "with io.open(fnameg, 'w') as json_file:\n",
    "    json_file.write(unicode(jstring))\n",
    "\n",
    "print \"Processing done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing done\n"
     ]
    }
   ],
   "source": [
    "#again some more processing of the files, by this point I've decided to not use figures for separate countries but instead\n",
    "# just focus on the aggregate figures for 'test nations' as a whole\n",
    "\n",
    "#This isn't even the final files I ended up using, I created another set of json files from this, with just a few minor changes\n",
    "#from these json files, and then another set of CSV files which are the ones I finally used to create the article's graphics\n",
    "\n",
    "with open('final_test_totals.json', 'r') as f:\n",
    "     test_totals_again = json.load(f)\n",
    "\n",
    "jdict = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))\n",
    "\n",
    "for nation in test_totals_again.keys():\n",
    "    #print nation\n",
    "    for year_band in test_totals_again[nation].keys():\n",
    "        #print year_band\n",
    "        for score_range in test_totals_again[nation][year_band].keys():\n",
    "            #print score_range\n",
    "            jdict[nation][year_band]['total_innings'] += test_totals_again[nation][year_band][score_range]\n",
    "        for score_range in test_totals_again[nation][year_band].keys():\n",
    "            jdict[nation][year_band][score_range] =\\\n",
    "                (test_totals_again[nation][year_band][score_range]/jdict[nation][year_band]['total_innings'])*100\n",
    "        \n",
    "jstring = json.dumps(jdict, indent=4, sort_keys=True)\n",
    "\n",
    "filenameg = 'final_test_totals_percent.json' \n",
    "fnameg = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenameg)\n",
    "\n",
    "with io.open(fnameg, 'w') as json_file:\n",
    "    json_file.write(unicode(jstring))\n",
    "\n",
    "print \"Processing done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing done\n"
     ]
    }
   ],
   "source": [
    "#same purpose as the code block above, just that this is for ODIs\n",
    "\n",
    "with open('final_odi_totals.json', 'r') as f:\n",
    "     odi_totals_again = json.load(f)\n",
    "\n",
    "jdict = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))\n",
    "\n",
    "for nation in odi_totals_again.keys():\n",
    "    #print nation\n",
    "    for year_band in odi_totals_again[nation].keys():\n",
    "        #print year_band\n",
    "        for score_range in odi_totals_again[nation][year_band].keys():\n",
    "            #print score_range\n",
    "            jdict[nation][year_band]['total_innings'] += odi_totals_again[nation][year_band][score_range]\n",
    "        for score_range in odi_totals_again[nation][year_band].keys():\n",
    "            jdict[nation][year_band][score_range] =\\\n",
    "                (odi_totals_again[nation][year_band][score_range]/jdict[nation][year_band]['total_innings'])*100\n",
    "        \n",
    "jstring = json.dumps(jdict, indent=4, sort_keys=True)\n",
    "\n",
    "filenameg = 'final_odi_totals_percent.json' \n",
    "fnameg = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenameg)\n",
    "\n",
    "with io.open(fnameg, 'w') as json_file:\n",
    "    json_file.write(unicode(jstring))\n",
    "\n",
    "print \"Processing done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing done\n"
     ]
    }
   ],
   "source": [
    "#i didn't end up using this file, but this looks at what % of all first innings test totals were in particular\n",
    "#score range, 0-200, 201-400 etc.\n",
    "\n",
    "with open('final_test_totals_percent.json', 'r') as f:\n",
    "     test_percent = json.load(f)\n",
    "        \n",
    "with open('final_test_totals.json', 'r') as g:\n",
    "     test_totals = json.load(g)\n",
    "\n",
    "jdict = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))\n",
    "\n",
    "for year_band in test_percent['Test_nations'].keys():\n",
    "    jdict[year_band]['0-200']['percent'] = test_percent['Test_nations'][year_band]['0-200']\n",
    "    jdict[year_band]['201-400']['percent'] = test_percent['Test_nations'][year_band]['201-400']\n",
    "    jdict[year_band]['401-600']['percent'] = test_percent['Test_nations'][year_band]['401-600']\n",
    "    jdict[year_band]['600-plus']['percent'] = test_percent['Test_nations'][year_band]['600 plus']\n",
    "\n",
    "for year_band in test_totals['Test_nations'].keys():\n",
    "    jdict[year_band]['0-200']['total'] = test_totals['Test_nations'][year_band]['0-200']\n",
    "    jdict[year_band]['201-400']['total'] = test_totals['Test_nations'][year_band]['201-400']\n",
    "    jdict[year_band]['401-600']['total'] = test_totals['Test_nations'][year_band]['401-600']\n",
    "    jdict[year_band]['600-plus']['total'] = test_totals['Test_nations'][year_band]['600 plus']\n",
    "    \n",
    "\n",
    "jstring = json.dumps(jdict, indent=4, sort_keys=True)\n",
    "\n",
    "filenameg = 'again_test_totals.json' \n",
    "fnameg = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenameg)\n",
    "\n",
    "with io.open(fnameg, 'w') as json_file:\n",
    "    json_file.write(unicode(jstring))\n",
    "\n",
    "print \"Processing done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing done\n"
     ]
    }
   ],
   "source": [
    "#calculates the no. of tests between a double or a triple century on average\n",
    "# decided later to not use this metric\n",
    "\n",
    "\n",
    "with open('final_test_indiv_scores.json', 'r') as f:\n",
    "     test_indiv = json.load(f)\n",
    "        \n",
    "with open('final_draw_percent.json', 'r') as g:\n",
    "     test_sum = json.load(g)\n",
    "\n",
    "jdict = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))\n",
    "\n",
    "first_band = [\"1997\",\"1998\",\"1999\",\"2000\",\"2001\"]\n",
    "second_band = [\"2002\",\"2003\",\"2004\",\"2005\",\"2006\"]\n",
    "third_band = [\"2007\",\"2008\",\"2009\",\"2010\",\"2011\"]\n",
    "fourth_band = [\"2012\",\"2013\",\"2014\",\"2015\",\"2016\"]\n",
    "\n",
    "for year in test_sum.keys():\n",
    "    if year in first_band:\n",
    "        jdict['1997-2001']['200-299']['tests'] += test_sum[year]['total_tests']\n",
    "        jdict['1997-2001']['300 & over']['tests'] += test_sum[year]['total_tests']\n",
    "    elif year in second_band:\n",
    "        jdict['2002-2006']['200-299']['tests'] += test_sum[year]['total_tests']\n",
    "        jdict['2002-2006']['300 & over']['tests'] += test_sum[year]['total_tests']\n",
    "    elif year in third_band:\n",
    "        jdict['2007-2011']['200-299']['tests'] += test_sum[year]['total_tests']\n",
    "        jdict['2007-2011']['300 & over']['tests'] += test_sum[year]['total_tests']\n",
    "    elif year in fourth_band:\n",
    "        jdict['2012-2016']['200-299']['tests'] += test_sum[year]['total_tests']\n",
    "        jdict['2012-2016']['300 & over']['tests'] += test_sum[year]['total_tests']\n",
    "        \n",
    "for year_band in test_indiv['Test_nations'].keys():        \n",
    "    jdict[year_band]['200-299']['total'] = test_indiv['Test_nations'][year_band]['200-299']\n",
    "    jdict[year_band]['200-299']['per_tests'] =\\\n",
    "        jdict[year_band]['200-299']['tests']/jdict[year_band]['200-299']['total']\n",
    "    \n",
    "    jdict[year_band]['300 & over']['total'] = test_indiv['Test_nations'][year_band]['300 & over']\n",
    "    jdict[year_band]['300 & over']['per_tests'] =\\\n",
    "        jdict[year_band]['300 & over']['tests']/jdict[year_band]['300 & over']['total']\n",
    "\n",
    "jstring = json.dumps(jdict, indent=4, sort_keys=True)\n",
    "\n",
    "filenameg = 'again_indiv_scores.json' \n",
    "fnameg = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenameg)\n",
    "\n",
    "with io.open(fnameg, 'w') as json_file:\n",
    "    json_file.write(unicode(jstring))\n",
    "\n",
    "print \"Processing done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing done\n"
     ]
    }
   ],
   "source": [
    "#this is just the json containing the run rate for test matches in a particular year\n",
    "#honestly this could have been in a CSV, there's a consistent depth and consistent no. of fields for everything\n",
    "#so there really isn't the need for a json anymore. Did the very same thing, five or six code blocks below when i converted\n",
    "#this into a CSV\n",
    "\n",
    "with open('final_test_run_wicket_rate.json', 'r') as f:\n",
    "     test_runs = json.load(f)\n",
    "        \n",
    "jdict = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "for year in test_runs['test_nations'].keys():\n",
    "    jdict[year]['runs'] = test_runs['test_nations'][year]['runs']\n",
    "    jdict[year]['run_rate'] = test_runs['test_nations'][year]['run_rate']\n",
    "    jdict[year]['overs'] = test_runs['test_nations'][year]['overs']\n",
    "\n",
    "jstring = json.dumps(jdict, indent=4, sort_keys=True)\n",
    "\n",
    "filenameg = 'again_run_rate.json' \n",
    "fnameg = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenameg)\n",
    "\n",
    "with io.open(fnameg, 'w') as json_file:\n",
    "    json_file.write(unicode(jstring))\n",
    "\n",
    "print \"Processing done\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing done\n"
     ]
    }
   ],
   "source": [
    "#this is a json that tells us on average how many overs we see between sixes\n",
    "#by this point I'd decided to concentrate on sixes as they represent a greater risk than 4s or boundaries\n",
    "#if the no. of sixes is going up, we can kind of see whether t20 cricket had something to do with it\n",
    "\n",
    "with open('final_test_four_six_rate.json', 'r') as f:\n",
    "     test_sixes = json.load(f)\n",
    "        \n",
    "jdict = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "for year in test_sixes['test_nations'].keys():\n",
    "    jdict[year]['sixes'] = test_sixes['test_nations'][year]['sixes']\n",
    "    jdict[year]['sixes_rate'] = test_sixes['test_nations'][year]['sixes_rate']\n",
    "    jdict[year]['overs'] = test_sixes['test_nations'][year]['overs']\n",
    "\n",
    "jstring = json.dumps(jdict, indent=4, sort_keys=True)\n",
    "\n",
    "filenameg = 'again_sixes_rate.json' \n",
    "fnameg = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenameg)\n",
    "\n",
    "with io.open(fnameg, 'w') as json_file:\n",
    "    json_file.write(unicode(jstring))\n",
    "\n",
    "print \"Processing done\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing done\n"
     ]
    }
   ],
   "source": [
    "#this looks at the percentage of dismissals that are run outs\n",
    "\n",
    "with open('final_test_run_out_rate.json', 'r') as f:\n",
    "     test_run_outs = json.load(f)\n",
    "        \n",
    "jdict = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "for year in test_run_outs['test_nations'].keys():\n",
    "    jdict[year]['run_outs'] = test_run_outs['test_nations'][year]['run_outs']\n",
    "    jdict[year]['run_out_rate'] = test_run_outs['test_nations'][year]['run_out_rate']\n",
    "    jdict[year]['wickets'] = test_run_outs['test_nations'][year]['wickets']\n",
    "\n",
    "jstring = json.dumps(jdict, indent=4, sort_keys=True)\n",
    "\n",
    "filenameg = 'again_run_outs.json' \n",
    "fnameg = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenameg)\n",
    "\n",
    "with io.open(fnameg, 'w') as json_file:\n",
    "    json_file.write(unicode(jstring))\n",
    "\n",
    "print \"Processing done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing done\n"
     ]
    }
   ],
   "source": [
    "#this looks at what percentage of overs bowled are maiden overs\n",
    "\n",
    "with open('final_test_maidens_rate.json', 'r') as f:\n",
    "     test_maidens = json.load(f)\n",
    "        \n",
    "jdict = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "for year in test_maidens['test_nations'].keys():\n",
    "    jdict[year]['maidens'] = test_maidens['test_nations'][year]['maidens']\n",
    "    jdict[year]['maiden_rate'] = test_maidens['test_nations'][year]['maiden_rate']\n",
    "    jdict[year]['overs'] = test_maidens['test_nations'][year]['overs']\n",
    "\n",
    "jstring = json.dumps(jdict, indent=4, sort_keys=True)\n",
    "\n",
    "filenameg = 'again_maidens.json' \n",
    "fnameg = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenameg)\n",
    "\n",
    "with io.open(fnameg, 'w') as json_file:\n",
    "    json_file.write(unicode(jstring))\n",
    "\n",
    "print \"Processing done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing done\n"
     ]
    }
   ],
   "source": [
    "#this looks at the rate at which wickets are falling, how many overs on average do we see between batsmen getting out\n",
    "\n",
    "with open('final_test_run_wicket_rate.json', 'r') as f:\n",
    "     test_wickets = json.load(f)\n",
    "        \n",
    "jdict = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "for year in test_wickets['test_nations'].keys():\n",
    "    jdict[year]['wickets'] = test_wickets['test_nations'][year]['wickets']\n",
    "    jdict[year]['wicket_rate'] = test_wickets['test_nations'][year]['wicket_rate']\n",
    "    jdict[year]['overs'] = test_wickets['test_nations'][year]['overs']\n",
    "\n",
    "jstring = json.dumps(jdict, indent=4, sort_keys=True)\n",
    "\n",
    "filenameg = 'again_wicket_freq.json' \n",
    "fnameg = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenameg)\n",
    "\n",
    "with io.open(fnameg, 'w') as json_file:\n",
    "    json_file.write(unicode(jstring))\n",
    "\n",
    "print \"Processing done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing done\n"
     ]
    }
   ],
   "source": [
    "#this looks at what percentage of matches in a year are draws\n",
    "\n",
    "with open('test_df_draws_by_year.json', 'r') as f:\n",
    "     test_df_draws = json.load(f)\n",
    "\n",
    "jdict = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "for year in test_df_draws.keys():\n",
    "    if year not in ['1990','2017']:\n",
    "        for result in test_df_draws[year].keys():\n",
    "            jdict[year]['total_tests'] += test_df_draws[year][result]\n",
    "            jdict[year]['draws'] = test_df_draws[year]['draw']\n",
    "            jdict[year]['draw_percent'] = (jdict[year]['draws']/jdict[year]['total_tests'])*100\n",
    "            \n",
    "            #note that the draw percent is actually calculated as many times as there are 'result' in [year].keys\n",
    "            #it's ok this time, as the result was still calculated correctly, but you might want to keep that out of the \n",
    "            #loop next time\n",
    "        \n",
    "jstring = json.dumps(jdict, indent=4, sort_keys=True)\n",
    "\n",
    "filenameg = 'again_draw_percent.json' \n",
    "fnameg = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenameg)\n",
    "\n",
    "with io.open(fnameg, 'w') as json_file:\n",
    "    json_file.write(unicode(jstring))\n",
    "\n",
    "print \"Processing done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing done\n"
     ]
    }
   ],
   "source": [
    "#this is a do over of a json file that we'd created earlier. what this does is aggregate the no. of double and triple centuries\n",
    "#instead of putting them in separate categories\n",
    "\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "from collections import defaultdict\n",
    "\n",
    "with open('again_indiv_scores.json', 'r') as f:\n",
    "     test_indiv_again = json.load(f)\n",
    "\n",
    "jdict = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "#this is for all invidiual scores 200 & over\n",
    "\n",
    "for year_band in test_indiv_again.keys():\n",
    "    for score_range in test_indiv_again[year_band].keys():\n",
    "        jdict[year_band]['tests'] = test_indiv_again[year_band][score_range]['tests']\n",
    "        jdict[year_band]['total'] += test_indiv_again[year_band][score_range]['total']\n",
    "        jdict[year_band]['per_tests'] =\\\n",
    "            jdict[year_band]['tests']/jdict[year_band]['total']\n",
    "            \n",
    "jstring = json.dumps(jdict, indent=4, sort_keys=True)\n",
    "\n",
    "filenameg = 'again_indiv_scores_v2.json' \n",
    "fnameg = os.path.join(os.path.dirname(os.path.realpath('__file__')), filenameg)\n",
    "\n",
    "with io.open(fnameg, 'w') as json_file:\n",
    "    json_file.write(unicode(jstring))\n",
    "\n",
    "print \"Processing done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing done\n"
     ]
    }
   ],
   "source": [
    "#actually wanted to create the visualisations using json files, but was running out of time, so I converted the jsons to CSVs\n",
    "#and used them instead\n",
    "\n",
    "#ended up making static graphics at the end, instead of creating some inventive visualisation of the data\n",
    "\n",
    "#note to self: if you're going to make something in d3, dont' start the day before the deadline!\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "for filex in os.listdir(os.curdir):\n",
    "    if filex.startswith('again_'):\n",
    "        if filex not in ['again_test_totals.json','again_indiv_scores.json']:\n",
    "            df=pd.read_json(filex)\n",
    "            df2 = df.transpose()\n",
    "            df2.index.name = 'year'\n",
    "            filey = filex.rstrip('.json') + '.csv'\n",
    "            df2.to_csv(filey)\n",
    "\n",
    "print \"Processing done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done and done\n"
     ]
    }
   ],
   "source": [
    "#this was a particularly complicated json file, so I ended up splitting these into four separate CSV files in order to make\n",
    "#graphics for it\n",
    "\n",
    "import io\n",
    "import json\n",
    "import csv\n",
    "\n",
    "with open('again_test_totals.json', 'r') as f:\n",
    "     test_tots_deux = json.load(f)\n",
    "\n",
    "headings = ['score','percent','total']\n",
    "\n",
    "for year_band in test_tots_deux.keys():\n",
    "    fileg = 'again_test_tot_' + year_band + '.csv'\n",
    "    with open(fileg, \"w\") as filem:\n",
    "        wr = csv.writer(filem, delimiter = ',' , quotechar = '\"',lineterminator='\\n')\n",
    "        wr.writerow(headings)\n",
    "    for score_range in test_tots_deux[year_band].keys():\n",
    "        row_list=[]\n",
    "        row_list.append(score_range)\n",
    "        row_list.append(test_tots_deux[year_band][score_range]['percent'])\n",
    "        row_list.append(test_tots_deux[year_band][score_range]['total'])\n",
    "        with open(fileg, \"a\") as filen:\n",
    "            wrz = csv.writer(filen, delimiter = ',' , quotechar = '\"',lineterminator='\\n' )\n",
    "            wrz.writerow(row_list)\n",
    "\n",
    "filen.close()\n",
    "print 'done and done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THE END, phew! :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
